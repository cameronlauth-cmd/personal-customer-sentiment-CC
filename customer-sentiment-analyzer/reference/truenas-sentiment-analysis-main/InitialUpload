PART1_VERSION = "1.5.1"

PART1_MODIFIED = "2024-12-16"

PART2_VERSION = "1.10.0"

PART2_MODIFIED = "2024-12-16"

PART3_VERSION = "1.3.6"

PART3_MODIFIED = "2024-12-16"

PART4_VERSION = "1.8.0"

PART4_MODIFIED = "2024-12-16"

PART5_VERSION = "1.6.1"

PART5_MODIFIED = "2024-12-16"

import pandas as pd

import numpy as np

import json

import time

import io

import matplotlib.pyplot as plt

from datetime import datetime

from reportlab.lib.pagesizes import letter

from reportlab.platypus import (
    SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak
)

from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle

from reportlab.lib.units import inch

from reportlab.lib import colors

from reportlab.lib.colors import HexColor

from reportlab.lib.enums import TA_CENTER, TA_JUSTIFY

from abacusai import AgentResponse, ApiClient, Blob

def extract_tech_info_from_message(message_text):
    """Extract tech name and role from email signature"""
    if pd.isna(message_text):
        return None
    
    import re
    msg_str = str(message_text).lower()
    
    # Look for common signature patterns at the end of message
    # Pattern: Name followed by role/title
    # Example: "John Doe, Tier 2 Software Support" or "Jane Smith\nSenior Support Engineer"
    
    # Split into lines and check last 10 lines for signature
    lines = message_text.split('\n')
    signature_lines = lines[-10:] if len(lines) > 10 else lines
    
    for i, line in enumerate(signature_lines):
        line_lower = line.lower()
        # Look for support role keywords
        if any(keyword in line_lower for keyword in ['tier 1', 'tier 2', 'tier 3', 'support engineer', 
                                                       'technical support', 'senior support', 'support specialist',
                                                       'software support', 'hardware support', 'escalation engineer']):
            # The name is likely in the line above or same line
            if i > 0:
                potential_name = signature_lines[i-1].strip()
                # Check if it looks like a name (2-4 words, capitalized, no @)
                if potential_name and '@' not in potential_name and len(potential_name.split()) <= 4:
                    # Clean up common email artifacts
                    potential_name = re.sub(r'(regards|thanks|best|sincerely),?\s*', '', potential_name, flags=re.IGNORECASE)
                    if len(potential_name) > 2:
                        return {
                            'name': potential_name.strip(),
                            'role': line.strip()
                        }
            
            # Check if name and role are on same line (e.g., "John Doe, Tier 2 Support")
            match = re.search(r'([A-Z][a-z]+\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s*[,\-]\s*', line)
            if match:
                return {
                    'name': match.group(1).strip(),
                    'role': line.strip()
                }
    
    return None

def build_tech_map_for_case(case_data):
    """Build a map of tech emails to their names/roles from message signatures"""
    tech_map = {}
    
    for _, row in case_data.iterrows():
        msg = row.get('Message', '')
        if pd.isna(msg):
            continue
        
        msg_str = str(msg)
        
        # Check if this looks like a tech response (has esupport@ or other @ixsystems.com)
        if '@ixsystems.com' in msg_str.lower():
            # Extract the email
            import re
            emails = re.findall(r'([\w\.-]+@ixsystems\.com)', msg_str, re.IGNORECASE)
            
            # Extract tech info from signature
            tech_info = extract_tech_info_from_message(msg_str)
            
            if tech_info and emails:
                for email in emails:
                    if email.lower() not in tech_map:
                        tech_map[email.lower()] = tech_info
    
    return tech_map

def load_and_prepare_data(excel_file, client):
    """Load Excel file and prepare dataframe"""
    if isinstance(excel_file, list):
        excel_file = excel_file[0]
    
    if not hasattr(excel_file, "contents"):
        raise ValueError("Expected file object with 'contents' attribute")
    
    try:
        df = pd.read_excel(io.BytesIO(excel_file.contents), engine="openpyxl")
        client.stream_message(f"✓ Loaded Excel file: {len(df)} records\n\n")
    except Exception as e:
        try:
            df = pd.read_excel(io.BytesIO(excel_file.contents), engine="xlrd")
            client.stream_message(f"✓ Loaded Excel file: {len(df)} records\n\n")
        except Exception as e2:
            raise ValueError(f"Failed to load Excel file: {str(e)}")
    
    # ✅ FIX: Define current_date at the beginning
    current_date = datetime.now()
    
    df.columns = df.columns.str.strip()
    column_mapping = {col.lower(): col for col in df.columns}
    
    required_columns_map = {
        "case number": ["case number", "case_number", "casenumber", "case no", "case#"],
        "customer name": ["account name", "account_name", "accountname", "customer name", "customer_name", "customername", "customer"],
        "message": ["text body", "text_body", "textbody", "message", "messages", "description", "comment", "text", "body"],
        "message date": ["message date", "message_date", "messagedate", "email date", "email_date", "date", "timestamp", "message timestamp"],
        "severity": ["severity", "priority", "level"],
        "support level": ["support level", "support_level", "supportlevel", "tier", "support tier"],
        "created date": ["created date", "created_date", "createddate", "date created", "create date", "opened date"],
        "last modified date": ["last modified date", "last_modified_date", "lastmodifieddate", "modified date", "updated date", "last updated"],
        "status": ["status", "state", "case status"],
        "case age days": ["case age days", "case_age_days", "caseagedays", "age days", "age_days", "agedays", "case age", "case_age"],
        "asset serial": ["asset serial", "asset_serial", "assetserial", "serial number", "serial_number", "system serial", "asset"],
    }
    
    actual_columns = {}
    for required_col, possible_names in required_columns_map.items():
        found = False
        for possible_name in possible_names:
            if possible_name in column_mapping:
                actual_columns[required_col] = column_mapping[possible_name]
                found = True
                break
        if not found:
            for col_lower, col_actual in column_mapping.items():
                if any(pn in col_lower for pn in possible_names):
                    actual_columns[required_col] = col_actual
                    found = True
                    break
        if not found and required_col not in ["case age days", "created date", "last modified date", "status", "support level", "message date", "asset serial"]:
            raise ValueError(f"Missing required column: {required_col}. Available: {list(df.columns)}")
    
    rename_dict = {
        actual_columns["case number"]: "Case Number",
        actual_columns["customer name"]: "Customer Name",
        actual_columns["message"]: "Message",
        actual_columns["severity"]: "Severity",
    }
    
    if "support level" in actual_columns:
        rename_dict[actual_columns["support level"]] = "Support Level"
    if "asset serial" in actual_columns:
        rename_dict[actual_columns["asset serial"]] = "Asset Serial"
    if "message date" in actual_columns:
        rename_dict[actual_columns["message date"]] = "Message Date"
    if "created date" in actual_columns:
        rename_dict[actual_columns["created date"]] = "Created Date"
    if "last modified date" in actual_columns:
        rename_dict[actual_columns["last modified date"]] = "Last Modified Date"
    if "case age days" in actual_columns:
        rename_dict[actual_columns["case age days"]] = "Case Age Days"
    if "status" in actual_columns:
        rename_dict[actual_columns["status"]] = "Status"
    
    df = df.rename(columns=rename_dict)
    
    df = df.dropna(subset=["Case Number", "Message"])
    df["Customer Name"] = df["Customer Name"].fillna("Unknown Customer")
    df["Severity"] = df["Severity"].fillna("S4")
    
    if "Support Level" not in df.columns:
        df["Support Level"] = "Unknown"
    else:
        df["Support Level"] = df["Support Level"].fillna("Unknown")
    
    if "Asset Serial" not in df.columns:
        df["Asset Serial"] = ""
    else:
        df["Asset Serial"] = df["Asset Serial"].fillna("")
    
    if "Message Date" in df.columns:
        try:
            df["Message Date"] = pd.to_datetime(df["Message Date"], errors="coerce")
        except:
            df["Message Date"] = pd.NaT
        df["Message Date"] = df["Message Date"].fillna(current_date)
    else:
        if "Created Date" in df.columns:
            df["Message Date"] = df["Created Date"]
        else:
            df["Message Date"] = current_date
    
    if "Status" not in df.columns:
        df["Status"] = "Unknown"
    else:
        df["Status"] = df["Status"].fillna("Unknown")
    
    if "Created Date" in df.columns:
        try:
            df["Created Date"] = pd.to_datetime(df["Created Date"], errors="coerce")
        except:
            df["Created Date"] = pd.NaT
        df["Created Date"] = df["Created Date"].fillna(current_date)
    else:
        df["Created Date"] = current_date
    
    if "Last Modified Date" in df.columns:
        try:
            df["Last Modified Date"] = pd.to_datetime(df["Last Modified Date"], errors="coerce")
        except:
            df["Last Modified Date"] = pd.NaT
        df["Last Modified Date"] = df["Last Modified Date"].fillna(current_date)
    else:
        df["Last Modified Date"] = current_date
    
    if "Case Age Days" in df.columns:
        df["case_age_days"] = pd.to_numeric(df["Case Age Days"], errors="coerce").fillna(0).astype(int)
    else:
        df["case_age_days"] = (current_date - df["Created Date"]).dt.days
        df["case_age_days"] = df["case_age_days"].fillna(0).astype(int)
    
    def extract_severity(severity_str):
        severity_str = str(severity_str).upper()
        if "S1" in severity_str:
            return "S1"
        elif "S2" in severity_str:
            return "S2"
        elif "S3" in severity_str:
            return "S3"
        elif "S4" in severity_str:
            return "S4"
        else:
            return "S4"
    
    df["Severity"] = df["Severity"].apply(extract_severity)
    
    return df, current_date

def detect_and_merge_case_relationships(df, client):
    """
    Detect duplicate relationships and merge child cases into parents
    Returns: Modified dataframe with merged cases
    """
    
    # Build case number to messages map
    case_groups = df.groupby('Case Number')
    
    parent_map = {}  # child_case_num → parent_case_num
    duplicate_cases = set()
    
    client.stream_message("\nDetecting case relationships (duplicates/escalations)...\n")
    
    # PHASE 1: Detect relationships
    for case_num, case_data in case_groups:
        messages_text = ' '.join(case_data['Message'].dropna().astype(str)).lower()
        
        # Look for duplicate indicators
        duplicate_indicators = [
            'duplicate case',
            'closing as duplicate',
            'duplicate ticket',
            'closed as duplicate',
            'this is a duplicate of',
            'related open case',
            'closing this case as a duplicate',
        ]
        
        is_duplicate = any(indicator in messages_text for indicator in duplicate_indicators)
        
        if is_duplicate:
            # Extract parent case reference
            import re
            patterns = [
                r'case\s*id:?\s*#?0*(\d{5,8})',
                r'ticket\s*#?0*(\d{5,8})',
                r'case\s*#0*(\d{5,8})',
                r'open\s+case\s+id:?\s*#?0*(\d{5,8})',
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, messages_text)
                for match in matches:
                    parent_num = int(match)
                    
                    # Normalize current case number for comparison
                    current_case_normalized = int(case_num) if isinstance(case_num, (int, str)) else case_num
                    
                    # Verify it's not self-reference
                    if parent_num == current_case_normalized:
                        continue
                    
                    # Check if parent exists (try multiple formats)
                    parent_exists = False
                    for check_num in [parent_num, str(parent_num), f"{parent_num:08d}"]:
                        if check_num in case_groups.groups:
                            parent_exists = True
                            parent_num = check_num  # Use the format that exists
                            break
                    
                    if parent_exists:
                        parent_map[case_num] = parent_num
                        duplicate_cases.add(case_num)
                        client.stream_message(f"  → Case {case_num} is duplicate of {parent_num}\n")
                        break
                    else:
                        # Parent not found - store for warning
                        if case_num not in parent_map:  # Only warn once per case
                            client.stream_message(f"  ⚠️ Case {case_num} references parent {parent_num} (not in dataset)\n")
                
                if case_num in parent_map:
                    break
    
    if len(duplicate_cases) == 0:
        client.stream_message("  No duplicate relationships detected\n\n")
        return df
    
    client.stream_message(f"\n  Found {len(duplicate_cases)} duplicate cases to merge\n\n")
    
    # PHASE 2: Merge duplicates into parents
    merged_rows = []
    
    for case_num, case_data in case_groups:
        if case_num in duplicate_cases:
            # This is a duplicate - merge into parent
            parent_num = parent_map[case_num]
            
            # Create escalation event marker
            escalation_event = pd.DataFrame([{
                'Case Number': parent_num,
                'Customer Name': case_data.iloc[0]['Customer Name'],
                'Message': f"[ESCALATION EVENT] Customer spawned duplicate case {case_num} indicating lack of response on this case. Messages from duplicate case follow below.",
                'Message Date': case_data['Message Date'].min(),
                'Severity': case_data.iloc[0]['Severity'],
                'Support Level': case_data.iloc[0]['Support Level'],
                'Created Date': case_data.iloc[0]['Created Date'],
                'Last Modified Date': case_data.iloc[0]['Last Modified Date'],
                'Status': case_data.iloc[0]['Status'],
                'Case Age Days': case_data.iloc[0].get('Case Age Days', 0),
            }])
            
            merged_rows.append(escalation_event)
            
            # Merge duplicate messages into parent (with prefix for clarity)
            duplicate_messages = case_data.copy()
            duplicate_messages['Case Number'] = parent_num
            duplicate_messages['Message'] = "[DUPLICATE CASE " + str(case_num) + "] " + duplicate_messages['Message'].astype(str)
            
            merged_rows.append(duplicate_messages)
            
            client.stream_message(f"  Merged {len(case_data)} messages from case {case_num} → parent {parent_num}\n")
        
        else:
            # Not a duplicate - include as-is
            merged_rows.append(case_data)
    
    # Combine all rows
    df_merged = pd.concat(merged_rows, ignore_index=True)
    
    # Sort by case number and message date for proper chronological order
    df_merged = df_merged.sort_values(['Case Number', 'Message Date'])
    
    original_case_count = len(case_groups)
    merged_case_count = df_merged['Case Number'].nunique()
    
    client.stream_message(f"\n✓ Merge complete:\n")
    client.stream_message(f"  Original cases: {original_case_count}\n")
    client.stream_message(f"  After merging: {merged_case_count}\n")
    client.stream_message(f"  Duplicates merged: {len(duplicate_cases)}\n\n")
    
    return df_merged

def build_account_intelligence_brief(case_analysis, asset_correlations=None, mode='full'):
    """
    Build account-level context to enhance individual case analysis
    mode: 'light' for quick scoring, 'full' for detailed timelines
    Returns: String to prepend to Claude prompts
    """
    
    if not case_analysis:
        return ""
    
    # Calculate account metrics
    total_cases = len(case_analysis)
    avg_frustration = np.mean([c['claude_analysis']['frustration_score'] for c in case_analysis])
    high_frust_count = len([c for c in case_analysis if c['claude_analysis']['frustration_score'] >= 7])
    systemic_count = len([c for c in case_analysis if c['claude_analysis']['issue_class'] == 'Systemic'])
    
    # Light mode (for quick scoring)
    if mode == 'light':
        brief = f"""
ACCOUNT CONTEXT (for prioritization):
• {total_cases} cases total, avg frustration {avg_frustration:.1f}/10
• {high_frust_count} high-frustration cases (≥7/10)
"""
        
        # Add temporal clustering if detected
        scores = [c['criticality_score'] for c in case_analysis]
        percentile_80 = np.percentile(scores, 80) if len(scores) > 0 else 0
        
        recent_concerning = []
        current_date = pd.Timestamp.now()
        cutoff = current_date - pd.Timedelta(days=60)
        
        for case in case_analysis:
            if case['criticality_score'] >= percentile_80 and case['criticality_score'] >= 140:
                try:
                    case_data = case.get('case_data')
                    if case_data is not None and not case_data.empty:
                        last_msg = case_data['Message Date'].max()
                        if last_msg >= cutoff:
                            recent_concerning.append(case['case_number'])
                except:
                    pass
        
        if len(recent_concerning) >= 2:
            brief += f"• ⚠️ PATTERN: {len(recent_concerning)} concerning cases in last 60 days - account deteriorating\n"
        
        brief += "\nConsider these patterns when assessing priority.\n"
        return brief
    
    # Full mode (for detailed timelines)
    else:
        brief = f"""
ACCOUNT-LEVEL INTELLIGENCE:
This case exists within broader account patterns. Use this context to inform your analysis:

ACCOUNT HEALTH SUMMARY:
• Total active cases: {total_cases}
• Average frustration level: {avg_frustration:.1f}/10
• High frustration cases: {high_frust_count}
• Systemic issues identified: {systemic_count}
"""
        
        # Temporal clustering detection
        scores = [c['criticality_score'] for c in case_analysis]
        percentile_80 = np.percentile(scores, 80) if len(scores) > 0 else 0
        
        recent_concerning = []
        current_date = pd.Timestamp.now()
        cutoff = current_date - pd.Timedelta(days=60)
        
        for case in case_analysis:
            if case['criticality_score'] >= percentile_80 and case['criticality_score'] >= 140:
                try:
                    case_data = case.get('case_data')
                    if case_data is not None and not case_data.empty:
                        last_msg = case_data['Message Date'].max()
                        if last_msg >= cutoff:
                            recent_concerning.append(case['case_number'])
                except:
                    pass
        
        if len(recent_concerning) >= 2:
            case_list = ', '.join(map(str, recent_concerning[:5]))
            brief += f"""
• ⚠️ TEMPORAL CLUSTERING DETECTED: {len(recent_concerning)} concerning cases in last 60 days
  Cases: {case_list}
  Pattern: Rapid deterioration or systemic problem affecting account
"""
        
        # Asset patterns (if available)
        if asset_correlations:
            recurring = asset_correlations.get('recurring_serials', [])
            if recurring:
                brief += f"\nASSET-LEVEL PATTERNS:\n"
                for asset in recurring[:3]:  # Top 3 problematic assets
                    case_nums = ', '.join(map(str, [c['case_number'] for c in asset['cases']]))
                    brief += f"• {asset['serial']} ({asset['component_type']}) - {asset['case_count']} cases: {case_nums}\n"
                    if asset['is_refurb']:
                        brief += f"  Note: Refurbished component ({asset['refurb_level']}) - higher failure risk\n"
                
            refurb_count = asset_correlations.get('refurb_case_count', 0)
            if refurb_count > 0:
                refurb_pct = (refurb_count / total_cases * 100)
                brief += f"\n• {refurb_pct:.0f}% of cases involve refurbished components\n"
        
        brief += """
Use these account-level patterns to provide context in your analysis - they help explain whether this 
case is an isolated incident or part of a larger systemic issue.
"""
        
        return brief

def run_claude_analysis(df, analysis_context, client):
    """Run Claude 3.5 Haiku analysis on all cases with message-by-message scoring"""
    unique_cases = df["Case Number"].unique()
    total_cases = len(unique_cases)
    
    # Identify customer name for account-level analysis
    customer_name = df["Customer Name"].iloc[0] if len(df) > 0 else "Unknown Customer"
    
    client.stream_message("="*70 + "\n")
    client.stream_message(f"STAGE 1: CLAUDE 3.5 HAIKU ANALYSIS\n")
    client.stream_message(f"Account: {customer_name}\n")
    client.stream_message(f"Analyzing {total_cases} cases for this customer\n")
    client.stream_message("="*70 + "\n\n")
    
    case_analysis = []
    issue_categories = {}
    support_level_distribution = {}
    claude_statistics = {
        "total_analyzed": 0,
        "high_frustration": 0,
        "medium_frustration": 0,
        "low_frustration": 0,
        "no_frustration": 0,
        "avg_frustration_score": 0,
        "total_frustration_score": 0,
        "api_errors": 0,
        "analysis_time_seconds": 0,
        "total_messages_analyzed": 0,
        "frustrated_messages_count": 0,
    }
    
    start_time = time.time()
    
    for idx, case_num in enumerate(unique_cases, 1):
        case_data = df[df["Case Number"] == case_num].copy()
        
        if idx % 5 == 0 or idx == 1:
            progress_pct = (idx / total_cases) * 100
            client.stream_message(f"[{idx}/{total_cases}] ({progress_pct:.1f}%) Claude analyzing...\n")
        
        first_row = case_data.iloc[0]
        customer_name = str(first_row["Customer Name"])
        severity = first_row["Severity"]
        created_date = first_row["Created Date"]
        last_modified = first_row["Last Modified Date"]
        status = str(first_row["Status"])
        case_age = int(first_row["case_age_days"])
        
        support_level_raw = str(first_row["Support Level"]).upper()
        if "GOLD" in support_level_raw:
            support_level = "Gold"
        elif "SILVER" in support_level_raw:
            support_level = "Silver"
        elif "BRONZE" in support_level_raw:
            support_level = "Bronze"
        elif "BASIC" in support_level_raw or "M-F" in support_level_raw or "8-5" in support_level_raw:
            support_level = "Basic"
        else:
            support_level = "Unknown"
        
        if support_level in support_level_distribution:
            support_level_distribution[support_level] += 1
        else:
            support_level_distribution[support_level] = 1
        
        interaction_count = len(case_data)
        
        case_data_sorted = case_data.sort_values('Message Date')
        case_messages = case_data_sorted["Message"].tolist()
        case_dates = case_data_sorted["Message Date"].tolist()
        
        # Build full message text for later use
        all_messages_text = "\n\n---MESSAGE---\n\n".join([
            f"[{case_dates[i].strftime('%b %d, %Y %I:%M %p') if isinstance(case_dates[i], pd.Timestamp) else 'Date Unknown'}] Msg {i+1}: {str(msg)}"
            for i, msg in enumerate(case_messages)
            if not pd.isna(msg)
        ])
        
        # ============= HYBRID MESSAGE-BY-MESSAGE ANALYSIS =============
        # Analyze each message individually, then combine scores
        message_scores = []
        messages_to_analyze = []
        
        # Prepare messages for batch analysis (limit each to 2000 chars)
        for i, msg in enumerate(case_messages):
            if pd.isna(msg):
                continue
            msg_str = str(msg).strip()
            if len(msg_str) > 2000:
                msg_str = msg_str[:2000] + "..."
            messages_to_analyze.append({
                'index': i + 1,
                'date': case_dates[i].strftime('%b %d, %Y') if isinstance(case_dates[i], pd.Timestamp) else 'Unknown',
                'text': msg_str
            })
        
        # Create a batch prompt for analyzing all messages at once
        messages_json = json.dumps(messages_to_analyze, indent=2)
        
        claude_prompt = f"""Analyze EACH message in this support case individually for frustration level.

CASE CONTEXT:
Customer: {customer_name}
Support Level: {support_level} tier
Case Duration: {case_age} days
Total Messages: {interaction_count}
Severity: {severity}

{analysis_context}

MESSAGES TO ANALYZE:
{messages_json}

IMPORTANT: Analyze EACH message independently for frustration signals.

For each message, assess the frustration level (0-10) based on:
- 0: Neutral/positive, thankful, satisfied
- 1-3: Minor concern, patient inquiry, polite follow-up
- 4-6: Growing impatience, disappointment, concern about timeline
- 7-8: Clear frustration, questioning competence, escalation threats
- 9-10: Extreme anger, trust broken, threats to leave/legal action

Respond with a JSON structure for EACH message:
[
  {{"msg": 1, "score": X, "reason": "brief reason"}},
  {{"msg": 2, "score": Y, "reason": "brief reason"}},
  ...
]

Then provide overall assessment:
ISSUE_CLASS: [What type of problem is this?]
- Systemic: Overall system not meeting performance/reliability expectations
- Environmental: Issues with how system fits in their environment (integration, compatibility)
- Component: Specific hardware/software component problem
- Procedural: Configuration issue, user error, or knowledge gap

RESOLUTION_OUTLOOK: [How likely is permanent resolution?]
- Challenging: May require significant changes or have no clear fix
- Manageable: Can be resolved but may take time/effort
- Straightforward: Clear path to resolution

KEY_PHRASE: [Most concerning customer statement from any message]"""

        try:
            claude_response = client.evaluate_prompt(
                prompt=claude_prompt,
                system_message="You are analyzing customer support messages for frustration patterns. Evaluate EACH message independently for emotional signals, then identify overall patterns. Be precise and objective in scoring individual messages.",
                llm_name="CLAUDE_V3_5_HAIKU",
            )
            
            claude_content = claude_response.content.strip()
            
            # Parse message scores from JSON response
            message_scores = []
            try:
                # Extract JSON array from response
                import re
                json_match = re.search(r'\[.*?\]', claude_content, re.DOTALL)
                if json_match:
                    scores_json = json_match.group()
                    message_scores = json.loads(scores_json)
                    claude_statistics["total_messages_analyzed"] += len(message_scores)
                    
                    # Count frustrated messages (score >= 4)
                    frustrated_count = len([s for s in message_scores if s.get('score', 0) >= 4])
                    claude_statistics["frustrated_messages_count"] += frustrated_count
            except:
                # Fallback if JSON parsing fails
                message_scores = []
            
            # Calculate metrics for hybrid scoring
            if message_scores:
                scores_only = [s.get('score', 0) for s in message_scores]
                
                # Core metrics
                average_score = np.mean(scores_only)
                peak_score = max(scores_only)
                frustrated_messages = [s for s in scores_only if s >= 4]
                frustration_frequency = len(frustrated_messages) / len(scores_only)
                
                # Apply hybrid formula
                if frustration_frequency < 0.2:  # Less than 20% frustrated
                    final_score = average_score  # Use average for isolated incidents
                elif frustration_frequency > 0.5:  # More than 50% frustrated
                    final_score = (peak_score * 0.7) + (average_score * 0.3)  # Weight peak heavily
                else:  # 20-50% frustrated
                    final_score = (peak_score * 0.4) + (average_score * 0.6)  # Balanced
                
                # Round to nearest integer for consistency
                final_score = round(final_score)
                
                # Store additional metrics
                frustration_metrics = {
                    'average_score': round(average_score, 2),
                    'peak_score': peak_score,
                    'frustration_frequency': round(frustration_frequency * 100, 1),
                    'frustrated_message_count': len(frustrated_messages),
                    'total_messages': len(scores_only),
                    'message_scores': message_scores[:10]  # Store first 10 for reference
                }
            else:
                # Fallback to single analysis if message-by-message fails
                final_score = 5  # Default medium score
                frustration_metrics = {
                    'average_score': 5,
                    'peak_score': 5,
                    'frustration_frequency': 50,
                    'frustrated_message_count': 1,
                    'total_messages': len(messages_to_analyze),
                    'message_scores': []
                }
            
            # Parse other fields from response
            claude_analysis = {
                "frustration_score": min(10, max(0, final_score)),
                "frustration_metrics": frustration_metrics,
                "issue_class": "Procedural",  # Default to least severe
                "resolution_outlook": "Straightforward",  # Default to simplest
                "key_phrase": "",
                "analysis_model": "Claude 3.5 Haiku (Hybrid)",
                "analysis_successful": True,
            }
            
            # Parse remaining fields from the response
            for line in claude_content.split('\n'):
                line = line.strip()
                
                if line.startswith('ISSUE_CLASS:'):
                    class_text = line.replace('ISSUE_CLASS:', '').strip()
                    # Check for each class type
                    if 'Systemic' in class_text:
                        claude_analysis['issue_class'] = 'Systemic'
                    elif 'Environmental' in class_text:
                        claude_analysis['issue_class'] = 'Environmental'
                    elif 'Component' in class_text:
                        claude_analysis['issue_class'] = 'Component'
                    elif 'Procedural' in class_text:
                        claude_analysis['issue_class'] = 'Procedural'
                
                elif line.startswith('RESOLUTION_OUTLOOK:'):
                    outlook_text = line.replace('RESOLUTION_OUTLOOK:', '').strip()
                    if 'Challenging' in outlook_text:
                        claude_analysis['resolution_outlook'] = 'Challenging'
                    elif 'Manageable' in outlook_text:
                        claude_analysis['resolution_outlook'] = 'Manageable'
                    elif 'Straightforward' in outlook_text:
                        claude_analysis['resolution_outlook'] = 'Straightforward'
                
                elif line.startswith('KEY_PHRASE:'):
                    phrase = line.replace('KEY_PHRASE:', '').strip()
                    if phrase.lower() != "none":
                        claude_analysis['key_phrase'] = phrase.strip('"').strip("'")
            
            # Extract excerpt for key phrase if found
            claude_excerpt = None
            if claude_analysis.get('key_phrase') and claude_analysis['key_phrase']:
                phrase = claude_analysis['key_phrase']
                phrase_lower = phrase.lower()
                
                for msg in case_messages:
                    if pd.isna(msg):
                        continue
                    msg_str = str(msg).strip()
                    msg_lower = msg_str.lower()
                    
                    if phrase_lower in msg_lower:
                        phrase_pos = msg_lower.find(phrase_lower)
                        start = max(0, phrase_pos - 250)
                        end = min(len(msg_str), phrase_pos + len(phrase) + 250)
                        
                        excerpt_text = msg_str[start:end].strip()
                        if start > 0:
                            excerpt_text = "..." + excerpt_text
                        if end < len(msg_str):
                            excerpt_text = excerpt_text + "..."
                        
                        excerpt_lower = excerpt_text.lower()
                        phrase_start = excerpt_lower.find(phrase_lower)
                        
                        if phrase_start != -1:
                            before = excerpt_text[:phrase_start]
                            matched = excerpt_text[phrase_start:phrase_start + len(phrase)]
                            after = excerpt_text[phrase_start + len(phrase):]
                            claude_excerpt = f'{before}<font color="#EA580C"><b>{matched}</b></font>{after}'
                        else:
                            claude_excerpt = excerpt_text
                        
                        break
            
            claude_analysis['excerpt'] = claude_excerpt
            claude_statistics["total_analyzed"] += 1
            claude_statistics["total_frustration_score"] += claude_analysis['frustration_score']
            
            if claude_analysis['frustration_score'] >= 7:
                claude_statistics["high_frustration"] += 1
            elif claude_analysis['frustration_score'] >= 4:
                claude_statistics["medium_frustration"] += 1
            elif claude_analysis['frustration_score'] >= 1:
                claude_statistics["low_frustration"] += 1
            else:
                claude_statistics["no_frustration"] += 1
            
        except Exception as e:
            claude_analysis = {
                "frustration_score": 0,
                "frustration_metrics": {
                    'average_score': 0,
                    'peak_score': 0,
                    'frustration_frequency': 0,
                    'frustrated_message_count': 0,
                    'total_messages': len(messages_to_analyze),
                    'message_scores': []
                },
                "issue_class": "Unknown",
                "resolution_outlook": "Unknown",
                "key_phrase": "",
                "excerpt": None,
                "analysis_model": "Claude 3.5 Haiku (Error)",
                "analysis_successful": False,
            }
            claude_statistics["api_errors"] += 1
        
        issue_category = claude_analysis.get('issue_class', 'Unknown')
        if issue_category in issue_categories:
            issue_categories[issue_category] += 1
        else:
            issue_categories[issue_category] = 1
        
        customer_engagement_ratio = 0.6 if interaction_count > 2 else 0.3
        
        # Build tech map from message signatures
        tech_map = build_tech_map_for_case(case_data)
        
        # Extract asset serial from first row
        asset_serial_raw = str(first_row.get("Asset Serial", "")).strip()
        
        case_analysis.append({
            "case_number": int(case_num),
            "customer_name": customer_name,
            "severity": severity,
            "support_level": support_level,
            "asset_serial": asset_serial_raw,  # NEW: Store asset serial
            "created_date": (
                created_date.strftime("%Y-%m-%d")
                if isinstance(created_date, pd.Timestamp)
                else str(created_date)
            ),
            "last_modified_date": (
                last_modified.strftime("%Y-%m-%d")
                if isinstance(last_modified, pd.Timestamp)
                else str(last_modified)
            ),
            "status": status,
            "case_age_days": case_age,
            "interaction_count": interaction_count,
            "customer_engagement_ratio": float(customer_engagement_ratio),
            "issue_category": issue_category,
            "claude_analysis": claude_analysis,
            "deepseek_analysis": None,
            "messages_full": all_messages_text,
            "case_data": case_data,
            "tech_map": tech_map,
        })
    
    claude_time = time.time() - start_time
    claude_statistics["analysis_time_seconds"] = claude_time
    claude_statistics["avg_frustration_score"] = (
        claude_statistics["total_frustration_score"] / claude_statistics["total_analyzed"]
        if claude_statistics["total_analyzed"] > 0 else 0
    )
    
    client.stream_message("\n" + "="*70 + "\n")
    client.stream_message(f"STAGE 1 COMPLETE: {claude_time:.1f} seconds\n")
    client.stream_message(f"  Analyzed: {claude_statistics['total_analyzed']} cases\n")
    client.stream_message(f"  Total Messages: {claude_statistics['total_messages_analyzed']}\n")
    client.stream_message(f"  Frustrated Messages: {claude_statistics['frustrated_messages_count']} ({claude_statistics['frustrated_messages_count']/max(1,claude_statistics['total_messages_analyzed'])*100:.1f}%)\n")
    client.stream_message(f"  High Frustration Cases: {claude_statistics['high_frustration']}\n")
    client.stream_message(f"  Average Score: {claude_statistics['avg_frustration_score']:.2f}/10\n")
    client.stream_message("="*70 + "\n\n")
    
    return case_analysis, claude_statistics, issue_categories, support_level_distribution, claude_time

def claude_frustration_to_points(claude_score):
    """
    Convert Claude's 0-10 frustration score to 0-100 points
    Uses logarithmic compression to create separation at high end
    
    Scale alignment:
    10 = 100pts (Return order/Legal action)
    9  = 96pts  (Won't renew/Relationship destroyed)
    8  = 91pts  (Trust broken/Adversarial)
    7  = 85pts  (High frustration/Strained)
    5  = 70pts  (Moderate concern)
    3  = 47pts  (Low frustration)
    1  = 11pts  (Minimal)
    0  = 0pts   (Satisfied)
    """
    if claude_score == 0:
        return 0
    
    # Logarithmic compression
    log_value = np.log(claude_score + 1) / np.log(11)
    
    # Scale to full 0-100 range
    points = log_value * 100
    
    return round(points, 1)

def extract_all_serials(case):
    """
    Best-effort serial extraction from field AND messages
    Returns: set of serial numbers found
    """
    import re
    serials = set()
    
    # Source 1: Asset Serial field (if exists and populated)
    asset_field = case.get('asset_serial', '')
    if asset_field and str(asset_field).strip() and str(asset_field) != 'nan':
        # Split by comma, semicolon, space, or combinations
        parts = re.split(r'[,;\s]+', str(asset_field))
        for part in parts:
            part = part.strip().upper()
            # Match serial pattern: A1-12345, R1-12345, C1-12345, etc.
            if re.match(r'^[A-Z]\d-\d{4,6}$', part):
                serials.add(part)
    
    # Source 2: Messages (fallback/supplement)
    messages = case.get('messages_full', '')
    if messages:
        # Look for serial patterns in text
        found = re.findall(r'\b([A-Z]\d-\d{4,6})\b', str(messages), re.IGNORECASE)
        serials.update([s.upper() for s in found])
    
    return serials

def analyze_asset_correlations(case_analysis, client):
    """
    Simple best-effort asset correlation
    No assumptions about structure - just correlate recurring serials
    """
    import re
    
    client.stream_message("\nAnalyzing asset correlations (best effort)...\n")
    
    serial_map = {}  # serial → list of cases
    
    # Build map of serials to cases
    for case in case_analysis:
        serials = extract_all_serials(case)
        
        for serial in serials:
            if serial not in serial_map:
                serial_map[serial] = []
            serial_map[serial].append({
                'case_number': case['case_number'],
                'criticality': case['criticality_score'],
                'frustration': case['claude_analysis']['frustration_score'],
                'issue_class': case['claude_analysis']['issue_class'],
                'created_date': case.get('created_date'),
            })
    
    # Find serials appearing in multiple cases
    recurring_serials = []
    for serial, cases in serial_map.items():
        if len(cases) >= 2:
            recurring_serials.append({
                'serial': serial,
                'case_count': len(cases),
                'cases': cases,
                'avg_criticality': np.mean([c['criticality'] for c in cases]),
                'is_refurb': serial.startswith(('R1-', 'R2-', 'R3-')),
                'refurb_level': serial[0:2] if serial.startswith('R') else 'A1',
                'component_type': 'Chassis' if serial.startswith('C') else 'Controller/Component',
            })
    
    # Sort by case count descending
    recurring_serials.sort(key=lambda x: x['case_count'], reverse=True)
    
    # Count refurb usage
    refurb_case_count = 0
    refurb_breakdown = {'R1': 0, 'R2': 0, 'R3': 0}
    
    for case in case_analysis:
        serials = extract_all_serials(case)
        has_refurb = False
        for serial in serials:
            if serial.startswith('R1-'):
                refurb_breakdown['R1'] += 1
                has_refurb = True
            elif serial.startswith('R2-'):
                refurb_breakdown['R2'] += 1
                has_refurb = True
            elif serial.startswith('R3-'):
                refurb_breakdown['R3'] += 1
                has_refurb = True
        if has_refurb:
            refurb_case_count += 1
    
    # Coverage stats
    cases_with_assets = len([c for c in case_analysis if extract_all_serials(c)])
    coverage_pct = (cases_with_assets / len(case_analysis) * 100) if case_analysis else 0
    
    client.stream_message(f"\n✓ Asset correlation complete:\n")
    client.stream_message(f"  Total serials tracked: {len(serial_map)}\n")
    client.stream_message(f"  Recurring serials (≥2 cases): {len(recurring_serials)}\n")
    client.stream_message(f"  Cases with asset data: {cases_with_assets}/{len(case_analysis)} ({coverage_pct:.0f}%)\n")
    if refurb_case_count > 0:
        client.stream_message(f"  Cases with refurb components: {refurb_case_count}\n")
    client.stream_message("\n")
    
    return {
        'recurring_serials': recurring_serials,
        'total_serials_tracked': len(serial_map),
        'serials_with_multiple_cases': len(recurring_serials),
        'refurb_case_count': refurb_case_count,
        'refurb_breakdown': refurb_breakdown,
        'total_cases': len(case_analysis),
        'cases_with_asset_data': cases_with_assets,
        'coverage_percent': round(coverage_pct, 1),
    }

def calculate_criticality_scores(case_analysis, client):
    """Calculate criticality scores for all cases"""
    client.stream_message("Calculating initial criticality scores...\n\n")
    
    for case in case_analysis:
        score = 0
        score_breakdown = {}
        
        # PRIMARY DRIVER: Claude's frustration score with logarithmic curve
        # Converts 0-10 to 0-100 points using log compression
        raw_frustration = case['claude_analysis']['frustration_score']
        frustration_points = claude_frustration_to_points(raw_frustration)
        score += frustration_points
        score_breakdown["claude_frustration"] = round(frustration_points, 1)
        
        severity_map = {"S1": 35, "S2": 25, "S3": 15, "S4": 5}
        severity_points = severity_map.get(case["severity"], 5)
        score += severity_points
        score_breakdown["technical_severity"] = severity_points
        
        msg_count = case["interaction_count"]
        if msg_count <= 2:
            volume_points = 30
        elif msg_count <= 5:
            volume_points = 20
        elif msg_count <= 10:
            volume_points = 10
        else:
            volume_points = 5
        score += volume_points
        score_breakdown["interaction_volume"] = volume_points
        
        engagement_points = 15 if case["customer_engagement_ratio"] > 0.6 else 0
        score += engagement_points
        score_breakdown["customer_engagement"] = engagement_points
        
        age = case["case_age_days"]
        if age >= 30:
            age_points = 10
        elif age >= 14:
            age_points = 7
        elif age >= 7:
            age_points = 3
        else:
            age_points = 0
        score += age_points
        score_breakdown["case_age"] = age_points
        
        # NEW: Issue Class scoring (replaces churn risk)
        issue_class = case['claude_analysis'].get('issue_class', 'Procedural')
        issue_class_map = {
            "Systemic": 30,      # Bumped up from 25 as requested
            "Environmental": 15,
            "Component": 10,
            "Procedural": 5,
            "Unknown": 0
        }
        issue_class_points = issue_class_map.get(issue_class, 0)
        score += issue_class_points
        score_breakdown["issue_class"] = issue_class_points
        
        # NEW: Resolution Outlook scoring
        resolution_outlook = case['claude_analysis'].get('resolution_outlook', 'Straightforward')
        resolution_map = {
            "Challenging": 15,
            "Manageable": 8,
            "Straightforward": 0,
            "Unknown": 0
        }
        resolution_points = resolution_map.get(resolution_outlook, 0)
        score += resolution_points
        score_breakdown["resolution_outlook"] = resolution_points
        
        support_level = case.get("support_level", "Unknown")
        if support_level == "Gold":
            support_level_points = 10
        elif support_level == "Silver":
            support_level_points = 5
        elif support_level == "Bronze":
            support_level_points = 3
        elif support_level == "Basic":
            support_level_points = 2
        else:
            support_level_points = 0
        score += support_level_points
        score_breakdown["support_level_priority"] = support_level_points
        
        case["initial_criticality_score"] = score
        case["score_breakdown"] = score_breakdown
        case["criticality_score"] = score
    
    case_analysis.sort(key=lambda x: x["initial_criticality_score"], reverse=True)
    return case_analysis

def classify_message_source(message_text):
    """Classify message as CUSTOMER or SUPPORT based on email sender"""
    if pd.isna(message_text):
        return 'UNKNOWN'
    
    msg_str = str(message_text).lower()
    
    # Simple rule: esupport@ = SUPPORT, everything else = CUSTOMER
    if 'esupport@' in msg_str:
        return 'SUPPORT'
    else:
        return 'CUSTOMER'

def analyze_response_ownership(case_data_sorted, severity):
    """Analyze who's responsible for each delay between messages"""
    
    messages_list = case_data_sorted["Message"].tolist()
    dates_list = case_data_sorted["Message Date"].tolist()
    
    response_analysis = []
    
    for i in range(len(messages_list) - 1):
        current_msg = messages_list[i]
        next_msg = messages_list[i + 1]
        current_date = dates_list[i]
        next_date = dates_list[i + 1]
        
        # Classify messages
        current_source = classify_message_source(current_msg)
        next_source = classify_message_source(next_msg)
        
        # Calculate gap in days
        if isinstance(current_date, pd.Timestamp) and isinstance(next_date, pd.Timestamp):
            gap_days = (next_date - current_date).days
        else:
            gap_days = 0
        
        # Determine responsibility
        if current_source == 'CUSTOMER' and gap_days > 0:
            # Customer sent last message, we took X days to respond
            responsible_party = 'SUPPORT'
            
            # Check SLA violations
            if severity == 'S1' and gap_days > 0:  # S1 = same-day response
                sla_violation = True
            elif severity == 'S2' and gap_days > 1:  # S2 = next business day
                sla_violation = True
            else:
                sla_violation = False
                
        elif current_source == 'SUPPORT' and gap_days > 0:
            # We sent last message, customer took X days to respond
            responsible_party = 'CUSTOMER'
            sla_violation = False  # Not our problem if customer doesn't respond
            
        else:
            responsible_party = 'UNKNOWN'
            sla_violation = False
        
        response_analysis.append({
            'message_index': i,
            'gap_days': gap_days,
            'responsible_party': responsible_party,
            'sla_violation': sla_violation,
            'current_source': current_source,
            'next_source': next_source
        })
    
    return response_analysis

def extract_frustrated_excerpts(case_data, frustrated_phrases):
    """Extract message excerpts with timestamps"""
    excerpts = []
    seen_excerpts = set()
    
    def is_likely_boilerplate(text):
        text_lower = text.lower().strip()
        if len(text_lower) < 10:
            return False
        boilerplate_patterns = [
            'subject:', 're:', 'fwd:', 'from:', 'sent:', 'to:',
            'please do not reply', 'unsubscribe', 'confidential',
        ]
        return any(pattern in text_lower for pattern in boilerplate_patterns)
    
    def normalize_text(text):
        normalized = text.lower().strip()
        normalized = ' '.join(normalized.split())
        return normalized.replace('!', '.').replace('?', '.')
    
    for phrase in frustrated_phrases:
        if not phrase or len(phrase) < 3:
            continue
        
        phrase_lower = phrase.lower()
        
        for msg_idx, row in case_data.iterrows():
            msg = row["Message"]
            if pd.isna(msg):
                continue
            msg_str = str(msg).strip()
            msg_lower = msg_str.lower()
            
            if phrase_lower in msg_lower:
                excerpt_text = msg_str.strip()
                
                if is_likely_boilerplate(excerpt_text):
                    continue
                
                normalized_excerpt = normalize_text(excerpt_text)
                if normalized_excerpt in seen_excerpts:
                    continue
                
                is_duplicate = False
                for seen in seen_excerpts:
                    seen_words = set(seen.split())
                    excerpt_words = set(normalized_excerpt.split())
                    if len(seen_words) > 0:
                        similarity = len(seen_words & excerpt_words) / len(seen_words | excerpt_words)
                        if similarity > 0.8:
                            is_duplicate = True
                            break
                
                if is_duplicate:
                    continue
                
                seen_excerpts.add(normalized_excerpt)
                
                excerpt_lower = excerpt_text.lower()
                phrase_start = excerpt_lower.find(phrase_lower)
                
                if phrase_start != -1:
                    before = excerpt_text[:phrase_start]
                    matched = excerpt_text[phrase_start:phrase_start + len(phrase)]
                    after = excerpt_text[phrase_start + len(phrase):]
                    highlighted = f'{before}<font color="#DC2626"><b>{matched}</b></font>{after}'
                else:
                    highlighted = excerpt_text
                
                timestamp = None
                if "Created Date" in row and pd.notna(row["Created Date"]):
                    timestamp = row["Created Date"]
                elif "Last Modified Date" in row and pd.notna(row["Last Modified Date"]):
                    timestamp = row["Last Modified Date"]
                
                excerpts.append({
                    "phrase": phrase,
                    "excerpt": highlighted,
                    "source": "DeepSeek-detected",
                    "timestamp": timestamp,
                    "message_index": msg_idx
                })
                break
    
    excerpts_sorted = sorted(
        [e for e in excerpts if e.get('timestamp')], 
        key=lambda x: x['timestamp'] if isinstance(x['timestamp'], pd.Timestamp) else pd.Timestamp.min
    )
    excerpts_sorted.extend([e for e in excerpts if not e.get('timestamp')])
    
    return excerpts_sorted

def run_deepseek_quick_scoring(case_analysis, analysis_context, client, account_brief_light=''):
    """Stage 2A: Quick scoring pass on top 25 - analyze patterns without building full timelines"""
    top_n = min(25, len(case_analysis))
    top_cases = case_analysis[:top_n]
    
    client.stream_message("="*70 + "\n")
    client.stream_message(f"STAGE 2A: CLAUDE SONNET QUICK SCORING\n")
    client.stream_message(f"Scoring top {top_n} cases from this account\n")
    client.stream_message("="*70 + "\n\n")
    
    deepseek_statistics = {
        "total_scored": 0,
        "api_errors": 0,
        "analysis_time_seconds": 0,
    }
    
    start_time = time.time()
    
    for idx, case in enumerate(top_cases, 1):
        client.stream_message(f"[{idx}/{top_n}] Scoring Case #{case['case_number']}...\n")
        
        messages_for_deepseek = case['messages_full']
        
        # Build prompt with account context prepended (surgical addition)
        quick_prompt = f"""{account_brief_light}

Assess this customer support case for prioritization scoring.

CASE OVERVIEW:
Customer: {case['customer_name']}
Support Level: {case['support_level']}
Duration: {case['case_age_days']} days
Messages: {case['interaction_count']}
Claude Score: {case['claude_analysis']['frustration_score']}/10
Severity: {case['severity']}

{analysis_context}

MESSAGE HISTORY (chronological):
{messages_for_deepseek}

SCORING ASSESSMENT:
Analyze the communication patterns and provide:

FRUSTRATION_FREQUENCY: [What % of messages show customer frustration? 0-100]
RELATIONSHIP_DAMAGE_FREQUENCY: [What % of interactions damaged confidence? 0-100]
CUSTOMER_PRIORITY: [Critical/High/Medium/Low based on relationship risk]
JUSTIFICATION: [2-3 sentences explaining priority level]

Focus on pattern recognition for scoring, not detailed message-by-message analysis."""

        try:
            response = client.evaluate_prompt(
                prompt=quick_prompt,
                system_message="You are analyzing customer support cases for prioritization. Focus on identifying patterns and risk levels efficiently. Maintain objective, factual language.",
                llm_name="CLAUDE_V3_5_SONNET"
            )
            
            content = response.content.strip()
            
            scoring = {
                'frustration_frequency': 0,
                'damage_frequency': 0,
                'priority': 'Medium',
                'justification': '',
                'analysis_model': 'Claude 3.5 Sonnet Quick Scoring',
                'analysis_successful': True
            }
            
            for line in content.split('\n'):
                line_stripped = line.strip()
                if 'FRUSTRATION_FREQUENCY:' in line_stripped:
                    nums = [int(s) for s in line_stripped.split() if s.isdigit()]
                    if nums:
                        scoring['frustration_frequency'] = min(100, max(0, nums[0]))
                elif 'RELATIONSHIP_DAMAGE_FREQUENCY:' in line_stripped:
                    nums = [int(s) for s in line_stripped.split() if s.isdigit()]
                    if nums:
                        scoring['damage_frequency'] = min(100, max(0, nums[0]))
                elif 'CUSTOMER_PRIORITY:' in line_stripped:
                    for priority in ['Critical', 'High', 'Medium', 'Low']:
                        if priority in line_stripped:
                            scoring['priority'] = priority
                            break
                elif 'JUSTIFICATION:' in line_stripped:
                    scoring['justification'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
            
            # Calculate DeepSeek score using same formula
            frustration_rate = scoring['frustration_frequency'] / 100
            damage_rate = scoring['damage_frequency'] / 100
            
            deepseek_base = (frustration_rate * 100) + (damage_rate * 50)
            
            priority_bonus = {
                'Critical': 20,
                'High': 10,
                'Medium': 5,
                'Low': 0
            }.get(scoring['priority'], 0)
            
            deepseek_points = deepseek_base + priority_bonus
            
            case['deepseek_quick_scoring'] = scoring
            case['score_breakdown']['deepseek_quick_score'] = round(deepseek_points, 1)
            case['score_breakdown']['deepseek_frustration_rate'] = round(frustration_rate * 100, 1)
            case['score_breakdown']['deepseek_damage_rate'] = round(damage_rate * 100, 1)
            case['score_breakdown']['deepseek_priority_bonus'] = priority_bonus
            case['criticality_score'] += deepseek_points
            
            deepseek_statistics["total_scored"] += 1
            
            client.stream_message(f"  → Priority: {scoring['priority']}, Score: +{deepseek_points:.1f}pts\n")
            
        except Exception as e:
            client.stream_message(f"  ✗ Failed: {str(e)}\n")
            case['deepseek_quick_scoring'] = {
                'frustration_frequency': 0,
                'damage_frequency': 0,
                'priority': 'Medium',
                'justification': 'Scoring failed',
                'analysis_model': 'Claude 3.5 Sonnet (Error)',
                'analysis_successful': False
            }
            deepseek_statistics["api_errors"] += 1
        
        if idx % 5 == 0:
            time.sleep(0.3)
    
    deepseek_time = time.time() - start_time
    deepseek_statistics["analysis_time_seconds"] = deepseek_time
    
    client.stream_message("\n" + "="*70 + "\n")
    client.stream_message(f"STAGE 2A COMPLETE: {deepseek_time:.1f}s\n")
    client.stream_message(f"  Scored: {deepseek_statistics['total_scored']} cases\n")
    client.stream_message("="*70 + "\n\n")
    
    # Re-sort cases based on updated scores
    client.stream_message("Re-ranking cases based on DeepSeek scoring...\n")
    case_analysis.sort(key=lambda x: x["criticality_score"], reverse=True)
    client.stream_message("✓ Cases re-ranked\n\n")
    
    return deepseek_statistics, deepseek_time

def run_deepseek_detailed_timeline(case_analysis, analysis_context, client, account_brief_full='', asset_correlations=None):
    """Stage 2B: Build detailed timelines using hybrid absolute OR relative threshold"""
    
    # Hybrid timeline selection
    ABSOLUTE_THRESHOLD = 180  # Always concerning
    RELATIVE_THRESHOLD = 140  # Minimum for relative selection
    MAX_TIMELINE_CASES = 10
    MIN_TIMELINE_CASES = 3
    
    client.stream_message("="*70 + "\n")
    client.stream_message(f"STAGE 2B: CLAUDE SONNET DETAILED TIMELINES\n")
    client.stream_message(f"Selection: Hybrid (Absolute ≥{ABSOLUTE_THRESHOLD} OR Relative outlier)\n")
    client.stream_message("="*70 + "\n\n")
    
    # Calculate relative threshold (90th percentile)
    scores = [c['criticality_score'] for c in case_analysis]
    percentile_90 = np.percentile(scores, 90) if len(scores) > 0 else 0
    
    client.stream_message(f"Absolute threshold: ≥{ABSOLUTE_THRESHOLD}\n")
    client.stream_message(f"Relative threshold: Top 10% (≥{percentile_90:.1f}) AND ≥{RELATIVE_THRESHOLD}\n\n")
    
    # Select cases using hybrid logic
    selected_cases = []
    absolute_selected = []
    relative_selected = []
    
    for case in case_analysis:
        score = case['criticality_score']
        
        # Condition 1: Absolute threshold
        if score >= ABSOLUTE_THRESHOLD:
            if case not in selected_cases:
                selected_cases.append(case)
                absolute_selected.append(case)
        
        # Condition 2: Relative threshold (outlier for this account)
        elif score >= RELATIVE_THRESHOLD and score >= percentile_90:
            if case not in selected_cases:
                selected_cases.append(case)
                relative_selected.append(case)
    
    client.stream_message(f"Cases meeting absolute threshold (≥{ABSOLUTE_THRESHOLD}): {len(absolute_selected)}\n")
    client.stream_message(f"Cases meeting relative threshold (≥{RELATIVE_THRESHOLD} & top 10%): {len(relative_selected)}\n")
    client.stream_message(f"Total cases selected: {len(selected_cases)}\n\n")
    
    # Cap at MAX_TIMELINE_CASES
    if len(selected_cases) > MAX_TIMELINE_CASES:
        client.stream_message(f"Limiting to top {MAX_TIMELINE_CASES} for detailed analysis\n")
        selected_cases = selected_cases[:MAX_TIMELINE_CASES]
    
    # Ensure minimum of 3 cases for timelines (even if below threshold)
    MIN_TIMELINE_CASES = 3
    if len(selected_cases) < MIN_TIMELINE_CASES and len(case_analysis) >= MIN_TIMELINE_CASES:
        client.stream_message(f"Only {len(selected_cases)} cases meet criteria\n")
        client.stream_message(f"Ensuring minimum {MIN_TIMELINE_CASES} cases for timeline analysis\n")
        # Take top N cases regardless of threshold
        selected_cases = case_analysis[:MIN_TIMELINE_CASES]
    
    client.stream_message(f"Generating timelines: {len(selected_cases)} cases\n")
    client.stream_message("="*70 + "\n\n")
    
    # Handle case where account has fewer than 3 total cases
    if len(selected_cases) == 0:
        client.stream_message("✓ Account has no cases to analyze!\n")
        client.stream_message("="*70 + "\n\n")
        return {
            "total_analyzed": 0,
            "api_errors": 0,
            "analysis_time_seconds": 0,
            "absolute_threshold": ABSOLUTE_THRESHOLD,
            "relative_threshold": RELATIVE_THRESHOLD,
            "percentile_90": percentile_90,
            "cases_meeting_absolute": 0,
            "cases_meeting_relative": 0,
            "cases_analyzed": 0,
        }, 0
    
    deepseek_statistics = {
        "total_analyzed": 0,
        "api_errors": 0,
        "analysis_time_seconds": 0,
        "absolute_threshold": ABSOLUTE_THRESHOLD,
        "relative_threshold": RELATIVE_THRESHOLD,
        "percentile_90": percentile_90,
        "cases_meeting_absolute": len(absolute_selected),
        "cases_meeting_relative": len(relative_selected),
        "cases_analyzed": len(selected_cases),
    }
    
    start_time = time.time()
    
    for idx, case in enumerate(selected_cases, 1):
        client.stream_message(f"[{idx}/{len(selected_cases)}] Case #{case['case_number']} (Score: {case['criticality_score']:.1f})...\n")
        
        case_data = case['case_data']
        messages_for_deepseek = case['messages_full']
        
        # NEW: Analyze response ownership patterns
        case_data_sorted = case_data.sort_values('Message Date')
        response_ownership = analyze_response_ownership(case_data_sorted, case['severity'])
        
        # NEW: Build enhanced message history with ownership markers
        messages_list = case_data_sorted["Message"].tolist()
        dates_list = case_data_sorted["Message Date"].tolist()
        
        enhanced_messages = []
        for i, (msg, date) in enumerate(zip(messages_list, dates_list)):
            if pd.isna(msg):
                continue
            
            # Classify source
            source = classify_message_source(msg)
            if source == 'SUPPORT':
                prefix = "[SUPPORT]"
            elif source == 'CUSTOMER':
                prefix = "[CUSTOMER]"
            else:
                prefix = "[UNKNOWN]"
            
            # Add delay attribution if not first message
            delay_info = ""
            if i > 0 and i-1 < len(response_ownership):
                ownership = response_ownership[i-1]
                gap = ownership['gap_days']
                responsible = ownership['responsible_party']
                
                if gap > 0:
                    if responsible == 'SUPPORT':
                        delay_info = f" (⏱ {gap}d delay - SUPPORT responsible)"
                    elif responsible == 'CUSTOMER':
                        delay_info = f" (⏱ {gap}d delay - CUSTOMER not responding)"
            
            # Format date
            date_str = date.strftime('%b %d, %Y %I:%M %p') if isinstance(date, pd.Timestamp) else 'Date Unknown'
            
            enhanced_messages.append(
                f"{prefix}{delay_info} [{date_str}] Msg {i+1}: {str(msg)}"
            )
        
        # Join enhanced messages
        enhanced_message_history = "\n\n---MESSAGE---\n\n".join(enhanced_messages)
        
        # Intelligent truncation for Claude Sonnet (200K token context window)
        # With two-stage analysis, we can use much more context for timeline generation
        if len(enhanced_message_history) > 300000:
            enhanced_message_history = enhanced_message_history[:300000] + "\n\n[...additional messages truncated for timeline analysis...]"
            client.stream_message(f"  ⚠ Messages truncated to 300K chars for context window\n")
        
        # Build asset-specific context for THIS case
        asset_context = ""
        if asset_correlations:
            case_serials = extract_all_serials(case)
            if case_serials:
                recurring_serials = asset_correlations.get('recurring_serials', [])
                for serial in case_serials:
                    # Check if this serial appears in other cases
                    for recurring in recurring_serials:
                        if recurring['serial'] == serial:
                            other_cases = [c for c in recurring['cases'] if c['case_number'] != case['case_number']]
                            if other_cases:
                                other_case_nums = ', '.join(map(str, [c['case_number'] for c in other_cases]))
                                asset_context += f"\n• Asset {serial} ({recurring['component_type']}) has appeared in {len(other_cases)} other case(s): {other_case_nums}"
                                asset_context += f"\n  This suggests recurring issue with this specific component"
                                if recurring['is_refurb']:
                                    asset_context += f" (Note: {recurring['refurb_level']} refurbished component)"
        
        if asset_context:
            asset_section = f"\nASSET HISTORY CONTEXT:{asset_context}\n"
        else:
            asset_section = ""
        
        # STEP 1: Generate the chronological timeline
        timeline_prompt = f"""{account_brief_full}
{asset_section}

Analyze this customer support case to assess relationship health and identify areas requiring attention.

CASE OVERVIEW:
Customer: {case['customer_name']}
Support Level: {case['support_level']}
Issue Severity: {case['severity']}
Case Status: {case['status']}
Case Duration: {case['case_age_days']} days
Message Count: {case['interaction_count']} messages
Initial Assessment: {case['claude_analysis']['frustration_score']}/10 frustration score

{analysis_context}

RESPONSE OWNERSHIP CONTEXT (CRITICAL):
Each message below is marked with [CUSTOMER] or [SUPPORT] and includes delay attribution.

INTERPRETING DELAYS:
- "(Xd delay - SUPPORT responsible)" = Customer sent last message, we took X days to respond
  → This IS a support quality issue if it violates SLA (S1 = same day, S2 = next business day)
  
- "(Xd delay - CUSTOMER not responding)" = We sent last message, customer took X days to respond
  → This is NOT a support quality issue - customer is not engaging
  → Multiple support follow-ups to silent customer = PROACTIVE support (POSITIVE)
  → Do NOT penalize support for customer non-responsiveness

AUTOMATED ALERTS (DEPRIORITIZE):
Many messages are automated system alerts from monitoring software (not human-initiated).
These are generated by TrueNAS systems automatically and do NOT represent customer frustration.

Common automated alert patterns:
- "Automated case creation acknowledgment"
- "System Alert:" or "Alert Notification"
- "Monitoring detected" or "Threshold exceeded"
- Generic template language
- No personalized content or specific questions

When you encounter automated alerts:
- Do NOT count them as frustrated customer messages
- Do NOT factor them heavily into relationship assessment
- Focus your analysis on HUMAN-INITIATED communications
- Automated alerts are informational, not emotional signals

COMPLETE MESSAGE HISTORY (chronological with ownership):
{enhanced_message_history}

ANALYSIS REQUIREMENTS:

Provide a chronological assessment of this case. When multiple messages represent routine activity on the same topic, group them by date range into digestible phases (typically 5-15 messages per group). When a message represents an inflection point (tone shift, escalation, resolution, frustration spike), analyze it individually. Break the case into phases based on meaningful events - each new problem, each resolution attempt, each escalation, or each significant delay should start a new timeline entry. All messages must be represented in the timeline.

IMPORTANT: If case status is "Closed", "Closed-NA", "Closed Duplicate", or "Closed-Test", provide a brief closure summary as your final timeline entry. If case status is anything else, the case is still active - suggest a recommended next action instead of a closure summary.

For message groups:
TIMELINE_ENTRY: [Messages X-Y - Date: MMM DD-DD, YYYY]
SUMMARY: [Factual description of what occurred during this phase]
CUSTOMER_TONE: [Observed tone across this period]
FRUSTRATION_DETECTED: [Yes/No]
FRUSTRATION_DETAIL: [If yes: specific language with context]
POSITIVE_ACTION_DETECTED: [Yes/No]
POSITIVE_ACTION_DETAIL: [If yes: specific language with context]
SUPPORT_QUALITY: [Assessment of support responses]
RELATIONSHIP_IMPACT: [Effect on customer confidence]

For critical moments:
TIMELINE_ENTRY: [Message X - Date: MMM DD, YYYY]
SUMMARY: [Factual description of what occurred in this interaction]
CUSTOMER_TONE: [Observed tone]
FRUSTRATION_DETECTED: [Yes/No]
FRUSTRATION_DETAIL: [If yes: specific language with context]
POSITIVE_ACTION_DETECTED: [Yes/No]
POSITIVE_ACTION_DETAIL: [If yes: specific language with context]
SUPPORT_QUALITY: [Assessment of support response]
RELATIONSHIP_IMPACT: [Effect on customer confidence]

FAILURE_PATTERN_DETECTED: [Yes/No - Is this message part of a pattern of repeated failures, escalations, or failed remediation attempts?
    Look for indicators: "second attempt", "third try", "additional replacement", "previous X failed", "escalation", "still not resolved", "another attempt"]
FAILURE_PATTERN_DETAIL: [If yes: Brief pattern description showing the sequence, e.g., "Third hardware replacement (controller failed, dual controller failed, now chassis)" or "Second escalation within 30 days after first attempt failed"]

Base all statements strictly on what appears in the messages. Direct quotes must be verbatim - your interpretation belongs in summaries, not in quotes. Do not invent names, dates, or details. Maintain objective, professional language suitable for executive review."""

        try:
            # STEP 1: Generate timeline
            timeline_response = client.evaluate_prompt(
                prompt=timeline_prompt,
                system_message="You are an enterprise customer experience analyst providing objective assessments of support interactions. Your role is to identify patterns, assess relationship health, and provide actionable insights based on communication analysis. Maintain a professional, analytical tone suitable for executive review.",
                llm_name="CLAUDE_V3_5_SONNET",
            )
            
            timeline_content = timeline_response.content.strip()
            
            client.stream_message(f"  Timeline generated, parsing entries...\n")
            
            # Parse timeline entries
            lines = timeline_content.split('\n')
            timeline_entries = []
            current_timeline_entry = None
            
            for i, line in enumerate(lines):
                line_stripped = line.strip()
                
                if not line_stripped:
                    continue
                
                # Remove markdown formatting
                line_cleaned = line_stripped.replace('**', '').replace('###', '').replace('##', '').replace('---', '').strip()
                
                if not line_cleaned:
                    continue
                
                if 'TIMELINE_ENTRY:' in line_cleaned:
                    if current_timeline_entry and current_timeline_entry.get('entry_label'):
                        timeline_entries.append(current_timeline_entry)
                    
                    entry_label = line_stripped.split('TIMELINE_ENTRY:', 1)[1].strip() if ':' in line_stripped else 'Unknown'
                    current_timeline_entry = {
                        'entry_label': entry_label,
                        'summary': '',
                        'customer_tone': '',
                        'frustration_detected': '',
                        'frustration_detail': '',
                        'support_quality': '',
                        'relationship_impact': '',
                        'message_excerpt': None
                    }
                    continue
                
                if current_timeline_entry is not None:
                    if 'SUMMARY:' in line_stripped:
                        current_timeline_entry['summary'] = line_stripped.split('SUMMARY:', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'CUSTOMER_TONE:' in line_stripped or 'CUSTOMER TONE:' in line_stripped:
                        current_timeline_entry['customer_tone'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'FRUSTRATION_DETECTED:' in line_stripped or 'FRUSTRATION DETECTED:' in line_stripped:
                        current_timeline_entry['frustration_detected'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'FRUSTRATION_DETAIL:' in line_stripped or 'FRUSTRATION DETAIL:' in line_stripped:
                        current_timeline_entry['frustration_detail'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'POSITIVE_ACTION_DETECTED:' in line_stripped or 'POSITIVE ACTION DETECTED:' in line_stripped:
                        current_timeline_entry['positive_action_detected'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'POSITIVE_ACTION_DETAIL:' in line_stripped or 'POSITIVE ACTION DETAIL:' in line_stripped:
                        current_timeline_entry['positive_action_detail'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'SUPPORT_QUALITY:' in line_stripped or 'SUPPORT QUALITY:' in line_stripped:
                        current_timeline_entry['support_quality'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'RELATIONSHIP_IMPACT:' in line_stripped or 'RELATIONSHIP IMPACT:' in line_stripped:
                        current_timeline_entry['relationship_impact'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    
                    # NEW: Parse failure pattern fields (surgical addition)
                    elif 'FAILURE_PATTERN_DETECTED:' in line_stripped or 'FAILURE PATTERN DETECTED:' in line_stripped:
                        current_timeline_entry['failure_pattern_detected'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
                    elif 'FAILURE_PATTERN_DETAIL:' in line_stripped or 'FAILURE PATTERN DETAIL:' in line_stripped:
                        current_timeline_entry['failure_pattern_detail'] = line_stripped.split(':', 1)[1].strip() if ':' in line_stripped else ''
                        continue
            
            if current_timeline_entry and current_timeline_entry.get('entry_label'):
                timeline_entries.append(current_timeline_entry)
            
            client.stream_message(f"  Parsed {len(timeline_entries)} timeline entries\n")
            
            # Validation: Check coverage ratio
            if len(timeline_entries) > 0:
                coverage_ratio = case['interaction_count'] / len(timeline_entries)
                if coverage_ratio > 15:
                    client.stream_message(f"  ⚠ Low coverage: {coverage_ratio:.1f} msgs/entry. Some messages may be missing.\n")
                elif coverage_ratio < 1:
                    client.stream_message(f"  ⚠ Over-detailed: {coverage_ratio:.1f} msgs/entry. Check for duplicate entries.\n")
                else:
                    client.stream_message(f"  ✓ Good coverage: {coverage_ratio:.1f} messages per timeline entry\n")
            
            # STEP 2: Generate executive summary based on the timeline
            if len(timeline_entries) > 0:
                timeline_summary = "\n\n".join([
                    f"TIMELINE_ENTRY: {entry.get('entry_label', 'Unknown')}\n"
                    f"Summary: {entry.get('summary', 'N/A')}\n"
                    f"Customer Tone: {entry.get('customer_tone', 'N/A')}\n"
                    f"Frustration Detected: {entry.get('frustration_detected', 'N/A')}\n"
                    f"Frustration Detail: {entry.get('frustration_detail', 'N/A')}\n"
                    f"Positive Action: {entry.get('positive_action_detected', 'N/A')}\n"
                    f"Support Quality: {entry.get('support_quality', 'N/A')}\n"
                    f"Relationship Impact: {entry.get('relationship_impact', 'N/A')}"
                    for entry in timeline_entries
                ])
                
                if len(timeline_summary) > 150000:
                    timeline_summary = timeline_summary[:150000] + "\n\n[...additional timeline entries truncated for summary generation...]"
                    client.stream_message(f"  ⚠ Timeline summary truncated to 150K chars for analysis\n")
                
                summary_prompt = f"""Based on the chronological timeline analysis of this support case, provide an executive summary.

CASE CONTEXT:
Customer: {case['customer_name']}
Support Level: {case['support_level']}
Issue Severity: {case['severity']}
Case Status: {case['status']}
Case Duration: {case['case_age_days']} days
Total Messages: {case['interaction_count']}

TIMELINE ANALYSIS:
{timeline_summary}

Provide executive summary using the exact format below. Do not use markdown formatting (no ##, **, or numbered lists). Each field should be on its own line:

PAIN_POINTS: [Key customer concerns based on communication patterns - 2-3 sentences]
SENTIMENT_TREND: [Evolution of customer sentiment throughout interaction - 1-2 sentences]
CRITICAL_INFLECTION_POINTS: [2-3 specific moments where relationship trajectory changed]
CUSTOMER_PRIORITY: [Urgency level based on analysis: Critical/High/Medium/Low]
RECOMMENDED_ACTION: [If case status is "Closed", "Closed-NA", "Closed Duplicate", or "Closed-Test": Provide brief closure summary. If case status is anything else: Suggest specific next action to advance or resolve the case - 1-2 sentences]
ROOT_CAUSE: [Primary factor contributing to current relationship state - 1-2 sentences]

Base your assessment on the timeline patterns identified above."""
                
                client.stream_message(f"  Generating executive summary from timeline...\n")
                
                summary_response = client.evaluate_prompt(
                    prompt=summary_prompt,
                    system_message="You are an enterprise customer experience analyst providing executive insights based on timeline analysis. Identify patterns, assess relationship health, and provide actionable recommendations. Maintain professional objectivity suitable for executive review.",
                    llm_name="CLAUDE_V3_5_SONNET",
                )
                
                summary_content = summary_response.content.strip()
            else:
                summary_content = ""
                client.stream_message(f"  ⚠ No timeline entries found, skipping summary\n")
            
            # Parse executive summary
            deepseek_analysis = {
                "pain_points": "",
                "sentiment_trend": "",
                "implicit_signals": "",
                "frustrated_phrases": [],
                "customer_priority": "Medium",
                "recommended_action": "",
                "root_cause": "",
                "critical_inflection_points": "",
                "timeline_entries": timeline_entries,
                "analysis_model": "Claude 3.5 Sonnet",
                "analysis_successful": True,
                "raw_timeline_response": timeline_content,
                "raw_summary_response": summary_content,
            }
            
            # Parse summary fields
            if summary_content:
                lines = summary_content.split('\n')
                current_field = None
                field_content = []
                
                for i, line in enumerate(lines):
                    line_stripped = line.strip()
                    
                    if not line_stripped:
                        continue
                    
                    line_cleaned = line_stripped.replace('**', '').replace('###', '').replace('##', '').replace('---', '').strip()
                    
                    if not line_cleaned:
                        continue
                    
                    if any(marker in line_cleaned.upper() for marker in ['PAIN_POINTS:', 'PAIN POINTS:', 'PAINPOINTS:']):
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        current_field = 'pain_points'
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            if after_colon and len(after_colon) > 5 and after_colon.lower() not in ['none', 'n/a', '-', 'unknown']:
                                field_content = [after_colon]
                            else:
                                field_content = []
                        else:
                            field_content = []
                        
                    elif 'SENTIMENT_TREND:' in line_cleaned or 'SENTIMENT TREND:' in line_cleaned:
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        current_field = 'sentiment_trend'
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            field_content = [after_colon] if after_colon else []
                        else:
                            field_content = []
                    
                    elif 'CRITICAL_INFLECTION_POINTS:' in line_cleaned or 'CRITICAL INFLECTION POINTS:' in line_cleaned:
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        current_field = 'critical_inflection_points'
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            field_content = [after_colon] if after_colon else []
                        else:
                            field_content = []
                        
                    elif 'CUSTOMER_PRIORITY:' in line_cleaned or 'CUSTOMER PRIORITY:' in line_cleaned:
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            if after_colon in ['Critical', 'High', 'Medium', 'Low']:
                                deepseek_analysis['customer_priority'] = after_colon
                        current_field = None
                        field_content = []
                        
                    elif 'RECOMMENDED_ACTION:' in line_cleaned or 'RECOMMENDED ACTION:' in line_cleaned:
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        current_field = 'recommended_action'
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            field_content = [after_colon] if after_colon else []
                        else:
                            field_content = []
                        
                    elif 'ROOT_CAUSE:' in line_cleaned or 'ROOT CAUSE:' in line_cleaned:
                        if current_field and field_content:
                            deepseek_analysis[current_field] = ' '.join(field_content).strip()
                        current_field = 'root_cause'
                        if ':' in line_cleaned:
                            after_colon = line_cleaned.split(':', 1)[1].strip()
                            field_content = [after_colon] if after_colon else []
                        else:
                            field_content = []
                    
                    elif current_field and line_cleaned:
                        if line_stripped.isupper() and len(line_stripped) > 20:
                            continue
                        if line_stripped in ['-', '•', '*', '—']:
                            continue
                        if ':' in line_cleaned and any(keyword in line_cleaned.upper() for keyword in [
                            'PAIN', 'SENTIMENT', 'CRITICAL', 'CUSTOMER', 'RECOMMENDED', 'ROOT'
                        ]):
                            continue
                        cleaned_line = line_stripped.lstrip('-•*→ ')
                        if cleaned_line and len(cleaned_line) > 3:
                            field_content.append(cleaned_line)
                
                if current_field and field_content:
                    deepseek_analysis[current_field] = ' '.join(field_content).strip()
            
            client.stream_message(f"    Executive summary fields:\n")
            client.stream_message(f"      Pain Points: {len(deepseek_analysis.get('pain_points', ''))} chars\n")
            client.stream_message(f"      Sentiment Trend: {len(deepseek_analysis.get('sentiment_trend', ''))} chars\n")
            client.stream_message(f"      Root Cause: {len(deepseek_analysis.get('root_cause', ''))} chars\n")
            
            if not any([deepseek_analysis.get('pain_points'), 
                       deepseek_analysis.get('sentiment_trend'),
                       deepseek_analysis.get('root_cause')]):
                client.stream_message(f"    ⚠ WARNING: Executive summary fields are empty!\n")
            
            # Extract message excerpts for timeline entries
            case_data_sorted = case_data.sort_values('Message Date')
            messages_list = case_data_sorted["Message"].tolist()
            
            for entry in timeline_entries:
                frustration_detail = entry.get('frustration_detail', '')
                
                import re
                quoted_text = re.findall(r'"([^"]+)"', frustration_detail)
                if not quoted_text:
                    quoted_text = re.findall(r"'([^']+)'", frustration_detail)
                
                if quoted_text and len(quoted_text[0]) > 10:
                    frustrated_quote = quoted_text[0]
                    quote_lower = frustrated_quote.lower()
                    
                    for msg in messages_list:
                        if pd.isna(msg):
                            continue
                        msg_str = str(msg).strip()
                        msg_lower = msg_str.lower()
                        
                        if quote_lower in msg_lower:
                            quote_pos = msg_lower.find(quote_lower)
                            start = max(0, quote_pos - 200)
                            end = min(len(msg_str), quote_pos + len(frustrated_quote) + 200)
                            
                            excerpt_text = msg_str[start:end].strip()
                            
                            if start > 0:
                                excerpt_text = "..." + excerpt_text
                            if end < len(msg_str):
                                excerpt_text = excerpt_text + "..."
                            
                            excerpt_text = excerpt_text.replace('&', '&amp;')
                            excerpt_text = excerpt_text.replace('<', '&lt;')
                            excerpt_text = excerpt_text.replace('>', '&gt;')
                            
                            escaped_quote = frustrated_quote.replace('&', '&amp;')
                            escaped_quote = escaped_quote.replace('<', '&lt;')
                            escaped_quote = escaped_quote.replace('>', '&gt;')
                            
                            excerpt_lower = excerpt_text.lower()
                            escaped_quote_lower = escaped_quote.lower()
                            quote_start = excerpt_lower.find(escaped_quote_lower)
                            
                            if quote_start != -1:
                                before = excerpt_text[:quote_start]
                                matched = excerpt_text[quote_start:quote_start + len(escaped_quote)]
                                after = excerpt_text[quote_start + len(escaped_quote):]
                                
                                frustration_detected = entry.get('frustration_detected', '').lower()
                                if 'yes' in frustration_detected:
                                    color = '#DC2626'
                                else:
                                    color = '#ea580c'
                                
                                highlighted = f'{before}<font color="{color}"><b>{matched}</b></font>{after}'
                                entry['message_excerpt'] = highlighted
                            else:
                                entry['message_excerpt'] = excerpt_text
                            
                            break
            
            # Extract positive excerpts
            for entry in timeline_entries:
                positive_detail = entry.get('positive_action_detail', '')
                
                import re
                quoted_text = re.findall(r'"([^"]+)"', positive_detail)
                if not quoted_text:
                    quoted_text = re.findall(r"'([^']+)'", positive_detail)
                
                if quoted_text and len(quoted_text[0]) > 10:
                    positive_quote = quoted_text[0]
                    quote_lower = positive_quote.lower()
                    
                    for msg in messages_list:
                        if pd.isna(msg):
                            continue
                        msg_str = str(msg).strip()
                        msg_lower = msg_str.lower()
                        
                        if quote_lower in msg_lower:
                            quote_pos = msg_lower.find(quote_lower)
                            start = max(0, quote_pos - 200)
                            end = min(len(msg_str), quote_pos + len(positive_quote) + 200)
                            
                            excerpt_text = msg_str[start:end].strip()
                            
                            if start > 0:
                                excerpt_text = "..." + excerpt_text
                            if end < len(msg_str):
                                excerpt_text = excerpt_text + "..."
                            
                            excerpt_text = excerpt_text.replace('&', '&amp;')
                            excerpt_text = excerpt_text.replace('<', '&lt;')
                            excerpt_text = excerpt_text.replace('>', '&gt;')
                            
                            escaped_quote = positive_quote.replace('&', '&amp;')
                            escaped_quote = escaped_quote.replace('<', '&lt;')
                            escaped_quote = escaped_quote.replace('>', '&gt;')
                            
                            excerpt_lower = excerpt_text.lower()
                            escaped_quote_lower = escaped_quote.lower()
                            quote_start = excerpt_lower.find(escaped_quote_lower)
                            
                            if quote_start != -1:
                                before = excerpt_text[:quote_start]
                                matched = excerpt_text[quote_start:quote_start + len(escaped_quote)]
                                after = excerpt_text[quote_start + len(escaped_quote):]
                                
                                highlighted = f'{before}<font color="#16a34a"><b>{matched}</b></font>{after}'
                                entry['positive_excerpt'] = highlighted
                            else:
                                entry['positive_excerpt'] = excerpt_text
                            
                            break
            
            excerpts = extract_frustrated_excerpts(case_data, deepseek_analysis['frustrated_phrases'])
            deepseek_analysis['excerpts'] = excerpts
            deepseek_analysis['total_excerpts_found'] = len(excerpts)
            
            case['deepseek_analysis'] = deepseek_analysis
            deepseek_statistics["total_analyzed"] += 1
            
            client.stream_message(f"  → Timeline: {len(timeline_entries)} entries | Priority: {deepseek_analysis['customer_priority']}\n")
            
        except Exception as e:
            client.stream_message(f"  ✗ Failed: {str(e)}\n")
            case['deepseek_analysis'] = {
                "pain_points": "Analysis failed",
                "sentiment_trend": "Unknown",
                "implicit_signals": "",
                "frustrated_phrases": [],
                "customer_priority": "Medium",
                "recommended_action": "Manual review required",
                "root_cause": "Analysis error",
                "critical_inflection_points": "",
                "timeline_entries": [],
                "analysis_model": "Claude 3.5 Sonnet (Error)",
                "analysis_successful": False,
                "excerpts": [],
                "total_excerpts_found": 0,
            }
            deepseek_statistics["api_errors"] += 1
        
        if idx % 5 == 0:
            time.sleep(0.3)
    
    deepseek_time = time.time() - start_time
    deepseek_statistics["analysis_time_seconds"] = deepseek_time
    
    client.stream_message("\n" + "="*70 + "\n")
    client.stream_message(f"STAGE 2B COMPLETE: {deepseek_time:.1f}s\n")
    client.stream_message(f"  Detailed timelines: {deepseek_statistics['total_analyzed']} cases\n")
    client.stream_message("="*70 + "\n\n")
    
    return deepseek_statistics, deepseek_time

def generate_all_charts(case_analysis, claude_statistics, issue_categories, 
                       severity_distribution, support_level_distribution):
    """Generate all visualization charts and return as bytes"""
    
    plt.style.use("default")
    
    def save_plot_to_bytes():
        buf = io.BytesIO()
        plt.savefig(buf, format="png", dpi=300, bbox_inches="tight", facecolor="white")
        buf.seek(0)
        return buf.getvalue()
    
    charts = {}
    
    # 1. Frustration Distribution
    plt.figure(figsize=(10, 7))
    labels = ['High\n(7-10)', 'Medium\n(4-6)', 'Low\n(1-3)', 'None\n(0)']
    sizes = [
        claude_statistics["high_frustration"],
        claude_statistics["medium_frustration"],
        claude_statistics["low_frustration"],
        claude_statistics["no_frustration"],
    ]
    colors_pie = ['#dc2626', '#ea580c', '#fbbf24', '#10b981']
    explode = (0.1, 0, 0, 0)
    
    wedges, texts, autotexts = plt.pie(
        sizes, labels=labels, autopct='%1.1f%%',
        colors=colors_pie, startangle=90, explode=explode,
        textprops={'fontsize': 12}
    )
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontsize(13)
    
    plt.title("Claude-Detected Frustration Distribution", fontsize=16, fontweight="bold", pad=20)
    charts['frustration_distribution'] = save_plot_to_bytes()
    plt.close()
    
    # 2. Issue Categories
    plt.figure(figsize=(12, 8))
    issue_items = sorted(list(issue_categories.items()), key=lambda x: x[1], reverse=True)[:10]
    if issue_items:
        # Truncate long category names to prevent rendering issues
        categories_truncated = []
        for cat in [item[0] for item in issue_items]:
            if len(str(cat)) > 30:
                categories_truncated.append(str(cat)[:27] + "...")
            else:
                categories_truncated.append(str(cat))
        
        counts = [item[1] for item in issue_items]
        
        bars = plt.bar(range(len(categories_truncated)), counts, color='#16a34a')
        plt.title("Issue Categories Distribution", fontsize=14, fontweight="bold", pad=20)
        plt.xlabel("Issue Category", fontsize=11)
        plt.ylabel("Number of Cases", fontsize=11)
        plt.xticks(range(len(categories_truncated)), categories_truncated, rotation=45, ha="right", fontsize=9)
        
        for i, bar in enumerate(bars):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
    charts['issue_categories'] = save_plot_to_bytes()
    plt.close()
    
    # 3. Score Breakdown (Stacked)
    plt.figure(figsize=(14, 8))
    top_10 = case_analysis[:10]
    case_labels = [f"{case['case_number']}" for case in top_10]
    
    age = [case["score_breakdown"]["case_age"] for case in top_10]
    engagement = [case["score_breakdown"]["customer_engagement"] for case in top_10]
    support = [case["score_breakdown"]["support_level_priority"] for case in top_10]
    volume = [case["score_breakdown"]["interaction_volume"] for case in top_10]
    severity = [case["score_breakdown"]["technical_severity"] for case in top_10]
    issue_class = [case["score_breakdown"]["issue_class"] for case in top_10]
    resolution = [case["score_breakdown"]["resolution_outlook"] for case in top_10]
    claude_frust = [case["score_breakdown"]["claude_frustration"] for case in top_10]
    sonnet_quick = [case["score_breakdown"].get("deepseek_quick_score", 0) for case in top_10]  # NEW
    
    x = np.arange(len(case_labels))
    width = 0.6
    
    p1 = plt.bar(x, age, width, label='Age', color='#D3D3D3')
    p2 = plt.bar(x, engagement, width, bottom=age, label='Engagement', color='#90EE90')
    bottom2 = np.array(age) + np.array(engagement)
    p3 = plt.bar(x, support, width, bottom=bottom2, label='Support Level', color='#9370DB')
    bottom3 = bottom2 + np.array(support)
    p4 = plt.bar(x, volume, width, bottom=bottom3, label='Message Volume', color='#4169E1')
    bottom4 = bottom3 + np.array(volume)
    p5 = plt.bar(x, severity, width, bottom=bottom4, label='Severity', color='#FFA500')
    bottom5 = bottom4 + np.array(severity)
    p6 = plt.bar(x, issue_class, width, bottom=bottom5, label='Issue Class', color='#FF6347')
    bottom6 = bottom5 + np.array(issue_class)
    p7 = plt.bar(x, resolution, width, bottom=bottom6, label='Resolution Outlook', color='#8B4513')
    bottom7 = bottom6 + np.array(resolution)
    p8 = plt.bar(x, claude_frust, width, bottom=bottom7, label='Claude Frustration (Log)', color='#DC143C')
    bottom8 = bottom7 + np.array(claude_frust)
    p9 = plt.bar(x, sonnet_quick, width, bottom=bottom8, label='Claude Sonnet Quick', color='#800080')  # NEW - Purple
    
    plt.title("Score Breakdown - Top 10 Cases (Hybrid)", fontsize=14, fontweight="bold", pad=20)
    plt.xlabel("Case Number", fontsize=11)
    plt.ylabel("Points Contributed", fontsize=11)
    plt.xticks(x, case_labels)
    plt.legend(loc='upper right')
    plt.tight_layout()
    charts['score_breakdown'] = save_plot_to_bytes()
    plt.close()
    
    # 4. Severity Distribution
    plt.figure(figsize=(10, 8))
    if severity_distribution:
        labels = list(severity_distribution.keys())
        sizes = list(severity_distribution.values())
        colors_sev = ['#DC143C', '#FFA500', '#4169E1', '#90EE90'][:len(labels)]
        
        wedges, texts, autotexts = plt.pie(
            sizes, labels=labels, autopct='%1.0f%%',
            colors=colors_sev, startangle=90,
            textprops={'fontsize': 11}
        )
        
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(12)
        
        plt.title("Severity Distribution", fontsize=14, fontweight="bold", pad=20)
    charts['severity_distribution'] = save_plot_to_bytes()
    plt.close()
    
    # 5. Support Level Distribution
    plt.figure(figsize=(10, 8))
    if support_level_distribution:
        labels = list(support_level_distribution.keys())
        sizes = list(support_level_distribution.values())
        colors_support = ['#FFD700', '#C0C0C0', '#CD7F32', '#808080'][:len(labels)]
        
        wedges, texts, autotexts = plt.pie(
            sizes, labels=labels, autopct='%1.0f%%',
            colors=colors_support, startangle=90,
            textprops={'fontsize': 11}
        )
        
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
            autotext.set_fontsize(12)
        
        plt.title("Support Level Distribution", fontsize=14, fontweight="bold", pad=20)
    charts['support_level_distribution'] = save_plot_to_bytes()
    plt.close()
    
    # 6. Top 25 Critical Cases
    plt.figure(figsize=(12, 10))
    top_25 = case_analysis[:25]
    top_25_labels = [f"{case['case_number']}" for case in top_25]
    top_25_scores = [case["criticality_score"] for case in top_25]
    
    # Color by CRITICALITY SCORE (consistent with Gantt chart)
    colors_bars = []
    for case in top_25:
        score = case["criticality_score"]
        if score >= 190:
            colors_bars.append('#DC2626')  # Red - Critical priority
        elif score >= 140:
            colors_bars.append('#ea580c')  # Orange - High priority
        else:
            colors_bars.append('#16a34a')  # Green - Lower priority
    
    y_pos = np.arange(len(top_25_labels))
    plt.barh(y_pos, top_25_scores, color=colors_bars)
    plt.yticks(y_pos, top_25_labels, fontsize=9)
    plt.xlabel("Criticality Score", fontsize=11)
    plt.title("Top 25 Critical Cases - Hybrid Scoring", fontsize=14, fontweight="bold", pad=20)
    plt.gca().invert_yaxis()
    
    for i, v in enumerate(top_25_scores):
        plt.text(v + 3, i, str(v), va='center', fontsize=9)
    
    # Add legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#DC2626', label='Critical Priority (≥190)'),
        Patch(facecolor='#ea580c', label='High Priority (140-189)'),
        Patch(facecolor='#16a34a', label='Lower Priority (<140)')
    ]
    plt.legend(handles=legend_elements, loc='lower right')
    
    plt.tight_layout()
    charts['top_25_critical'] = save_plot_to_bytes()
    plt.close()
    
    # 7. Frustration Trend Over Time (UPDATED - uses message activity dates)
    plt.figure(figsize=(12, 6))
    
    # Collect all messages with dates and frustration scores
    message_activity = []
    for case in case_analysis:
        try:
            case_data = case.get('case_data')
            if case_data is not None and not case_data.empty:
                for _, row in case_data.iterrows():
                    msg_date = row.get('Message Date')
                    if pd.notna(msg_date) and isinstance(msg_date, pd.Timestamp):
                        message_activity.append({
                            'date': msg_date,
                            'frustration': case['claude_analysis']['frustration_score']
                        })
        except:
            continue
    
    if len(message_activity) > 0:
        # Sort by date
        message_activity.sort(key=lambda x: x['date'])
        
        # Determine bucketing based on date range
        dates = [m['date'] for m in message_activity]
        date_range_days = (max(dates) - min(dates)).days
        
        if date_range_days > 90:
            bucket_format = '%Y-%m'
            def bucket_label(d): return d.strftime('%b %Y')
        elif date_range_days > 14:
            bucket_format = '%Y-W%U'
            def bucket_label(d): return f"Week {d.strftime('%b %d')}"
        else:
            bucket_format = '%Y-%m-%d'
            def bucket_label(d): return d.strftime('%b %d')
        
        # Group by period
        activity_by_period = {}
        for activity in message_activity:
            period_key = activity['date'].strftime(bucket_format)
            if period_key not in activity_by_period:
                activity_by_period[period_key] = []
            activity_by_period[period_key].append(activity['frustration'])
        
        # Calculate average frustration per period
        periods = sorted(activity_by_period.keys())
        avg_frustration = [np.mean(activity_by_period[p]) for p in periods]
        
        # Create labels
        period_labels = []
        for p in periods:
            try:
                if bucket_format == '%Y-W%U':
                    year, week = p.split('-W')
                    first_day = pd.to_datetime(f'{year}-01-01') + pd.Timedelta(weeks=int(week))
                    period_labels.append(bucket_label(first_day))
                else:
                    period_labels.append(bucket_label(pd.to_datetime(p if '-' in p else p + '-01')))
            except:
                period_labels.append(p)
        
        # Plot
        x_positions = range(len(periods))
        plt.plot(x_positions, avg_frustration, marker='o', linewidth=2, markersize=8, color='#DC2626')
        plt.axhline(y=7, color='#FFA500', linestyle='--', linewidth=1.5, label='High Frustration (7)')
        plt.axhline(y=4, color='#fbbf24', linestyle=':', linewidth=1.5, label='Medium Frustration (4)')
        
        plt.title("Customer Frustration Trend Over Time", fontsize=14, fontweight="bold", pad=20)
        plt.xlabel("Time Period", fontsize=11)
        plt.ylabel("Average Frustration Score", fontsize=11)
        plt.xticks(x_positions, period_labels, rotation=45, ha="right", fontsize=9)
        plt.ylim(0, 10)
        plt.legend(loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
    
    charts['frustration_trend'] = save_plot_to_bytes()
    plt.close()
    
    # 8. Case Timeline Gantt Chart (shows case duration and overlap)
    plt.figure(figsize=(14, 10))
    
    # Sort cases by first message date (chronological)
    def get_first_message_date(case):
        """Get the earliest message date from case data"""
        try:
            case_data = case.get('case_data')
            if case_data is not None and not case_data.empty:
                msg_dates = case_data['Message Date'].dropna()
                if len(msg_dates) > 0:
                    return msg_dates.min()
        except:
            pass
        return pd.to_datetime(case['created_date'])
    
    def get_last_message_date(case):
        """Get the latest message date from case data"""
        try:
            case_data = case.get('case_data')
            if case_data is not None and not case_data.empty:
                msg_dates = case_data['Message Date'].dropna()
                if len(msg_dates) > 0:
                    return msg_dates.max()
        except:
            pass
        # Fallback to last modified
        return pd.to_datetime(case['last_modified_date'])
    
    # Sort cases by first message date (newest first for recent activity focus)
    cases_sorted = sorted(case_analysis, key=get_first_message_date, reverse=True)
    
    # Show top 30 most recent cases
    display_cases = cases_sorted[:30]
    
    # Keep in reverse chronological order (newest first = top of chart, oldest last = bottom)
    # This creates a timeline reading down from top-left to bottom-right
    
    valid_cases = []
    for case in display_cases:
        try:
            start_date = get_first_message_date(case)
            end_date = get_last_message_date(case)
            
            # Validate dates
            if pd.notna(start_date) and pd.notna(end_date) and start_date <= end_date:
                valid_cases.append({
                    'case': case,
                    'start': start_date,
                    'end': end_date,
                    'duration_days': (end_date - start_date).days
                })
        except Exception as e:
            continue
    
    if len(valid_cases) > 0:
        for idx, case_info in enumerate(valid_cases):
            case = case_info['case']
            start = case_info['start']
            end = case_info['end']
            duration = case_info['duration_days']
            
            # Color by FINAL CRITICALITY SCORE (not just frustration)
            criticality = case['criticality_score']
            if criticality >= 190:
                color = '#DC2626'  # Red - Critical priority
                alpha = 0.9
            elif criticality >= 140:
                color = '#ea580c'  # Orange - High priority
                alpha = 0.8
            else:
                color = '#16a34a'  # Green - Lower priority
                alpha = 0.7
            
            # Draw horizontal bar from start to end
            # Use date2num to convert dates to matplotlib numbers
            from matplotlib.dates import date2num
            start_num = date2num(start)
            end_num = date2num(end)
            width = end_num - start_num
            
            # Ensure minimum width for visibility (at least 1 day)
            if width < 1:
                width = 1
            
            plt.barh(
                idx, 
                width,
                left=start_num,
                height=0.8,
                color=color,
                alpha=alpha,
                edgecolor='black',
                linewidth=0.5
            )
            
            # Add case number label on the left
            label = f"Case {case['case_number']}"
            if case['status'] not in ['Closed', 'Closed-NA', 'Closed Duplicate', 'Closed-Test']:
                label += " (Active)"
            
            # Put label inside the bar or to the left
            if width > 30:  # If bar is wide enough, put label inside
                plt.text(
                    start_num + width/2,
                    idx,
                    label,
                    va='center',
                    ha='center',
                    fontsize=7,
                    color='white',
                    fontweight='bold'
                )
            else:
                # Put label to the left of the bar
                plt.text(
                    start_num - 2,
                    idx,
                    label,
                    va='center',
                    ha='right',
                    fontsize=7
                )
        
        # Format x-axis as dates
        from matplotlib.dates import DateFormatter, MonthLocator
        ax = plt.gca()
        ax.xaxis.set_major_formatter(DateFormatter('%b %Y'))
        ax.xaxis.set_major_locator(MonthLocator(interval=2))  # Show every 2 months
        
        plt.xlabel('Date', fontsize=11)
        plt.ylabel('Cases (Newest to Oldest)', fontsize=11)
        plt.title('Case Timeline - Recent Activity (Top 30 Most Recent Cases)', fontsize=14, fontweight='bold', pad=20)
        plt.yticks([])  # Hide y-axis tick marks
        plt.grid(True, axis='x', alpha=0.3)
        plt.gcf().autofmt_xdate()  # Rotate date labels
        
        # Add legend with updated thresholds
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#DC2626', alpha=0.9, label='Critical Priority (Score ≥190)'),
            Patch(facecolor='#ea580c', alpha=0.8, label='High Priority (Score 140-189)'),
            Patch(facecolor='#16a34a', alpha=0.7, label='Lower Priority (Score <140)')
        ]
        plt.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
    
    charts['case_volume_trend'] = save_plot_to_bytes()
    plt.close()
    
    return charts

def calculate_temporal_clustering_penalty(case_analysis, lookback_days=60):
    """
    Detect if multiple concerning cases are clustered in recent time
    Returns: (penalty_multiplier, clustering_info)
    - penalty_multiplier: 0.0 to 1.0 (0 = no penalty, 1 = full penalty)
    - clustering_info: dict with details for reporting
    """
    
    current_date = pd.Timestamp.now()
    cutoff_date = current_date - pd.Timedelta(days=lookback_days)
    
    # Find cases that are:
    # 1. Concerning (≥140 AND in top 20% for this account)
    # 2. Recent (last 60 days)
    
    scores = [c['criticality_score'] for c in case_analysis]
    threshold_80th = np.percentile(scores, 80)  # Top 20%
    
    recent_concerning_cases = []
    for case in case_analysis:
        # Get case's most recent activity
        try:
            case_data = case.get('case_data')
            if case_data is not None and not case_data.empty:
                last_msg = case_data['Message Date'].max()
            else:
                last_msg = pd.to_datetime(case['last_modified_date'])
        except:
            continue
        
        # Is it concerning AND recent?
        is_concerning = (case['criticality_score'] >= 140 and 
                        case['criticality_score'] >= threshold_80th)
        is_recent = last_msg >= cutoff_date
        
        if is_concerning and is_recent:
            recent_concerning_cases.append({
                'case_number': case['case_number'],
                'score': case['criticality_score'],
                'last_activity': last_msg,
                'days_ago': (current_date - last_msg).days
            })
    
    # Calculate clustering penalty based on count and temporal proximity
    num_recent = len(recent_concerning_cases)
    
    if num_recent == 0:
        return 0.0, {'detected': False}
    
    elif num_recent == 1:
        # Single concerning case - mild penalty
        penalty = 0.1
        description = "1 concerning case in last 60 days"
    
    elif num_recent == 2:
        # Two concerning cases - check how close together
        dates = [c['last_activity'] for c in recent_concerning_cases[:2]]
        days_between = (max(dates) - min(dates)).days
        
        if days_between <= 14:  # Within 2 weeks
            penalty = 0.4
            description = f"2 concerning cases within {days_between} days - rapid deterioration"
        elif days_between <= 30:  # Within 1 month
            penalty = 0.25
            description = f"2 concerning cases within {days_between} days - concerning trend"
        else:
            penalty = 0.15
            description = f"2 concerning cases within {days_between} days - isolated incidents"
    
    elif num_recent >= 3:
        # Three or more - check clustering
        dates = [c['last_activity'] for c in recent_concerning_cases[:3]]
        days_span = (max(dates) - min(dates)).days
        
        if days_span <= 14:  # 3+ cases in 2 weeks
            penalty = 0.7
            description = f"{num_recent} concerning cases within {days_span} days - SEVERE deterioration"
        elif days_span <= 30:  # 3+ cases in 1 month
            penalty = 0.5
            description = f"{num_recent} concerning cases within {days_span} days - major concern"
        else:
            penalty = 0.3
            description = f"{num_recent} concerning cases within {days_span} days - ongoing issues"
    else:
        penalty = 0.0
        description = "No clustering detected"
    
    return penalty, {
        'detected': True,
        'penalty_multiplier': penalty,
        'recent_case_count': num_recent,
        'cases': recent_concerning_cases,
        'description': description,
        'lookback_days': lookback_days
    }

def calculate_catastrophic_override_weight(case, current_date):
    """
    Calculate override weight based on recency (0.0 to 1.0)
    1.0 = full override (recent disaster)
    0.0 = no override (ancient history)
    """
    # Get last activity date (last message or last modified)
    try:
        case_data = case.get('case_data')
        if case_data is not None and not case_data.empty:
            msg_dates = case_data['Message Date'].dropna()
            if len(msg_dates) > 0:
                last_msg_date = msg_dates.max()
            else:
                last_msg_date = pd.to_datetime(case['last_modified_date'])
        else:
            last_msg_date = pd.to_datetime(case['last_modified_date'])
    except:
        last_msg_date = pd.to_datetime(case['last_modified_date'])
    
    # Calculate days since last activity
    days_ago = (current_date - last_msg_date).days
    
    # Graduated decay based on recency
    if days_ago <= 90:  # 0-3 months
        return 1.0  # Full override - recent disaster
    elif days_ago <= 180:  # 3-6 months
        return 0.5  # Half override - fading concern
    elif days_ago <= 365:  # 6-12 months
        return 0.25  # Quarter override - old news
    else:  # >12 months
        return 0.0

def calculate_account_health_score(case_analysis, claude_statistics):
    """Calculate holistic account health score (0-100, higher = healthier) - STRICT version"""
    
    total_cases = len(case_analysis)
    
    # Handle edge case: no cases analyzed
    if total_cases == 0:
        return 0, {
            'frustration_component': 0,
            'high_frustration_penalty': 0,
            'critical_load_component': 0,
            'systemic_issues_component': 0,
            'resolution_complexity_component': 0
        }
    
    # Component 1: Average Frustration (0-30 points)
    avg_frustration = claude_statistics['avg_frustration_score']
    frustration_score = max(0, 30 - (avg_frustration * 3))
    # 0/10 = 30pts, 3/10 = 21pts, 5/10 = 15pts, 10/10 = 0pts
    
    # Component 2: High Frustration Cases (0-20 points) - PENALTY for ANY high frustration
    high_frustration_count = claude_statistics['high_frustration']  # Cases ≥7
    high_frustration_ratio = high_frustration_count / total_cases
    
    # Check for catastrophic cases (≥200) with recency weighting
    catastrophic_cases = [c for c in case_analysis if c['criticality_score'] >= 200]
    current_date = pd.Timestamp.now()
    
    if len(catastrophic_cases) > 0:
        # Calculate recency-weighted override
        override_weights = [calculate_catastrophic_override_weight(c, current_date) 
                           for c in catastrophic_cases]
        max_override = max(override_weights)  # Use worst case (most recent)
        
        # Apply graduated override
        normal_score = max(0, 20 - (high_frustration_ratio * 100))
        high_frustration_score = normal_score * (1 - max_override)
        # max_override = 1.0 (0-3mo) → 0 pts (full override)
        # max_override = 0.5 (3-6mo) → 10 pts (half penalty)
        # max_override = 0.25 (6-12mo) → 15 pts (quarter penalty)
        # max_override = 0.0 (>12mo) → 20 pts (no override)
    else:
        # No catastrophic cases - normal calculation
        high_frustration_score = max(0, 20 - (high_frustration_ratio * 100))
        # 0% = 20pts, 10% = 10pts, 20%+ = 0pts
        max_override = 0.0
    
    # Component 3: Critical Case Load (0-20 points) - Cases ≥180
    critical_count = len([c for c in case_analysis if c['criticality_score'] >= 180])
    critical_ratio = critical_count / total_cases
    
    # Use the same catastrophic override weight calculated above
    if max_override > 0:
        # Apply graduated override based on recency
        normal_score = max(0, 20 - (critical_ratio * 100))
        critical_score = normal_score * (1 - max_override)
    else:
        # No catastrophic cases - normal calculation
        critical_score = max(0, 20 - (critical_ratio * 100))
        # 0% = 20pts, 10% = 10pts, 20%+ = 0pts
    
    # Component 4: Systemic Issues (0-15 points)
    systemic_count = len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Systemic'])
    systemic_ratio = systemic_count / total_cases
    systemic_score = max(0, 15 - (systemic_ratio * 75))
    # 0% = 15pts, 10% = 7.5pts, 20%+ = 0pts
    
    # Component 5: Challenging Resolutions (0-15 points)
    challenging_count = len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Challenging'])
    challenging_ratio = challenging_count / total_cases
    challenging_score = max(0, 15 - (challenging_ratio * 75))
    # 0% = 15pts, 10% = 7.5pts, 20%+ = 0pts
    
    # Total health score (max 100) BEFORE temporal adjustment
    base_health_score = (frustration_score + high_frustration_score + 
                        critical_score + systemic_score + challenging_score)
    base_health_score = max(0, min(100, base_health_score))
    
    # NEW: Apply temporal clustering penalty
    temporal_penalty, temporal_info = calculate_temporal_clustering_penalty(case_analysis, lookback_days=60)
    
    if temporal_penalty > 0:
        # Apply temporal clustering penalty to base score
        health_score = base_health_score * (1 - temporal_penalty)
        health_score = round(max(0, min(100, health_score)), 1)
    else:
        health_score = round(base_health_score, 1)
    
    return health_score, {
        'frustration_component': round(frustration_score, 1),
        'high_frustration_penalty': round(high_frustration_score, 1),
        'critical_load_component': round(critical_score, 1),
        'systemic_issues_component': round(systemic_score, 1),
        'resolution_complexity_component': round(challenging_score, 1),
        'catastrophic_override_applied': max_override > 0,
        'catastrophic_override_weight': round(max_override, 2),
        'catastrophic_case_count': len(catastrophic_cases) if len(catastrophic_cases) > 0 else 0,
        'base_health_score': round(base_health_score, 1),
        'temporal_clustering_penalty': round(temporal_penalty, 2),
        'temporal_clustering_info': temporal_info
    }

def replace_tech_emails_with_names(text, tech_map):
    """Replace tech emails with their actual names from signatures"""
    if not text or not tech_map:
        return text
    
    import re
    
    # Find all @ixsystems.com emails in the text
    emails = re.findall(r'([\w\.-]+@ixsystems\.com)', text, re.IGNORECASE)
    
    for email in emails:
        email_lower = email.lower()
        if email_lower in tech_map:
            tech_info = tech_map[email_lower]
            # Replace with name and role
            replacement = f"{tech_info['name']} ({tech_info['role'].split(',')[0].strip()})"
            text = text.replace(email, replacement)
        else:
            # Fallback to generic if we don't have their info
            text = text.replace(email, '[Support Tech]')
    
    return text

def generate_pdf_report(case_analysis, claude_statistics, deepseek_statistics, 
                        claude_time, deepseek_time, total_time, current_date, charts, asset_correlations=None):
    """Generate comprehensive PDF report with case interaction timelines and asset correlation"""
    
    # Get version information from all parts
    code_versions = {
        "Part 1 (Core)": f"v{PART1_VERSION} ({PART1_MODIFIED})",
        "Part 2 (Claude Sonnet)": f"v{PART2_VERSION} ({PART2_MODIFIED}) - Timeline analysis",
        "Part 3 (Viz/PDF)": f"v{PART3_VERSION} ({PART3_MODIFIED})",
        "Part 4 (Main)": f"v{PART4_VERSION} ({PART4_MODIFIED})",
    }
    
    pdf_buffer = io.BytesIO()
    doc = SimpleDocTemplate(
        pdf_buffer,
        pagesize=letter,
        rightMargin=0.75*inch,
        leftMargin=0.75*inch,
        topMargin=0.75*inch,
        bottomMargin=0.75*inch,
    )
    
    styles = getSampleStyleSheet()
    
    # Custom styles
    if 'ReportTitle' not in styles:
        styles.add(ParagraphStyle(
            name='ReportTitle',
            parent=styles['Heading1'],
            fontSize=24,
            leading=30,
            textColor=HexColor("#1e40af"),
            fontName='Helvetica-Bold',
            alignment=TA_CENTER,
            spaceAfter=10,
        ))
    
    if 'ReportSubtitle' not in styles:
        styles.add(ParagraphStyle(
            name='ReportSubtitle',
            parent=styles['Normal'],
            fontSize=16,
            leading=20,
            textColor=HexColor("#1e40af"),
            fontName='Helvetica-Bold',
            alignment=TA_CENTER,
            spaceAfter=30,
        ))
    
    if 'SectionHeading' not in styles:
        styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=styles['Heading2'],
            fontSize=16,
            leading=20,
            textColor=HexColor("#1e40af"),
            fontName='Helvetica-Bold',
            spaceAfter=12,
            spaceBefore=20,
        ))
    
    if 'SubHeading' not in styles:
        styles.add(ParagraphStyle(
            name='SubHeading',
            parent=styles['Heading3'],
            fontSize=13,
            leading=16,
            textColor=HexColor("#1e40af"),
            fontName='Helvetica-Bold',
            spaceAfter=10,
            spaceBefore=15,
        ))
    
    if 'BodyText' not in styles:
        styles.add(ParagraphStyle(
            name='BodyText',
            parent=styles['Normal'],
            fontSize=10,
            leading=14,
            alignment=TA_JUSTIFY,
        ))
    
    story = []
    top_25_critical = case_analysis[:25]
    
    # ========== SECTION 1: COVER PAGE (BRANDED SCORECARD STYLE) ==========
    
    # TrueNAS brand colors
    truenas_blue = HexColor("#0095D5")
    truenas_dark = HexColor("#1a1a1a")
    
    # Header bar with logo
    story.append(Spacer(1, 0.5*inch))
    
    # Try to load logo if available
    try:
        # Logo should be in /mnt/user-data/uploads/
        logo_path = "/mnt/user-data/uploads/TrueNAS_Open_Enterprise_Logo_Virtual_Background.png"
        logo = Image(logo_path)
        # Crop to just the logo portion (upper right ~600px)
        logo._restrictSize(2.5*inch, 0.6*inch)
        story.append(logo)
    except:
        # Fallback if logo not available
        header_style = ParagraphStyle(
            'HeaderBar',
            parent=styles['Normal'],
            fontSize=14,
            textColor=truenas_blue,
            fontName='Helvetica-Bold',
            alignment=TA_CENTER,
        )
        story.append(Paragraph("TrueNAS Support Analytics", header_style))
    
    story.append(Spacer(1, 0.3*inch))
    
    # Thin blue line separator
    line_table = Table([['']], colWidths=[7*inch])
    line_table.setStyle(TableStyle([
        ('LINEABOVE', (0, 0), (-1, 0), 3, truenas_blue),
    ]))
    story.append(line_table)
    story.append(Spacer(1, 0.8*inch))
    
    # Title
    story.append(Paragraph("CUSTOMER ACCOUNT HEALTH ASSESSMENT", styles['ReportTitle']))
    story.append(Spacer(1, 0.3*inch))
    
    # Customer name and date
    customer_name_display = case_analysis[0]['customer_name'] if case_analysis else "Unknown Customer"
    story.append(Paragraph(customer_name_display, styles['ReportSubtitle']))
    story.append(Spacer(1, 0.1*inch))
    
    date_style = ParagraphStyle(
        'DateStyle',
        parent=styles['Normal'],
        fontSize=11,
        textColor=colors.grey,
        alignment=TA_CENTER,
    )
    story.append(Paragraph(f"Analysis Date: {current_date.strftime('%B %d, %Y')}", date_style))
    story.append(Spacer(1, 0.6*inch))
    
    # Calculate health metrics for cover
    temp_health_score, temp_score_breakdown = calculate_account_health_score(case_analysis, claude_statistics)
    
    # Determine color for health score
    if temp_health_score >= 85:
        health_color = "#16a34a"
        health_status = "Healthy - Low Risk"
    elif temp_health_score >= 70:
        health_color = "#fbbf24"
        health_status = "Stable - Minor Concerns"
    elif temp_health_score >= 60:
        health_color = "#ea580c"
        health_status = "At Risk - Moderate Concerns"
    else:
        health_color = "#DC2626"
        health_status = "Critical - High Renewal Risk"
    
    # Health score box with colored background
    health_score_large_style = ParagraphStyle(
        'HealthScoreLarge', 
        parent=styles['Normal'], 
        fontSize=48,
        leading=50,
        fontName='Helvetica-Bold',
        alignment=TA_CENTER
    )
    
    health_status_style = ParagraphStyle(
        'HealthStatusStyle',
        parent=styles['Normal'],
        fontSize=14,
        fontName='Helvetica-Bold',
        alignment=TA_CENTER,
        textColor=HexColor(health_color)
    )
    
    health_box_data = [
        [Paragraph("ACCOUNT HEALTH SCORE", styles['SubHeading'])],
        [Spacer(1, 0.1*inch)],
        [Paragraph(f"<b>{temp_health_score:.0f}/100</b>", health_score_large_style)],
        [Spacer(1, 0.05*inch)],
        [Paragraph(health_status, health_status_style)],
    ]
    
    health_box = Table(health_box_data, colWidths=[5*inch])
    health_box.setStyle(TableStyle([
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('BOX', (0, 0), (-1, -1), 2, HexColor(health_color)),
        ('BACKGROUND', (0, 0), (-1, -1), HexColor("#f9fafb")),
        ('TOPPADDING', (0, 0), (-1, -1), 10),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 10),
    ]))
    story.append(health_box)
    story.append(Spacer(1, 0.5*inch))
    
    # At-a-glance metrics in three boxes
    total_cases = len(case_analysis)
    avg_frustration = claude_statistics['avg_frustration_score']
    critical_cases_count = len([c for c in case_analysis if c['criticality_score'] >= 180])
    high_frustration_count = claude_statistics['high_frustration']
    timelines_count = deepseek_statistics.get('total_analyzed', 0)
    
    # Create three metric boxes side by side
    metric_box_style = ParagraphStyle('MetricBox', parent=styles['Normal'], fontSize=9, alignment=TA_CENTER)
    metric_value_style = ParagraphStyle('MetricValue', parent=styles['Normal'], fontSize=16, fontName='Helvetica-Bold', alignment=TA_CENTER)
    
    metrics_data = [
        [
            Paragraph("Cases Analyzed", metric_box_style),
            Paragraph("Avg Frustration", metric_box_style),
            Paragraph("Critical Cases", metric_box_style),
        ],
        [
            Paragraph(f"<b>{total_cases}</b>", metric_value_style),
            Paragraph(f"<b>{avg_frustration:.2f}/10</b>", metric_value_style),
            Paragraph(f"<b>{critical_cases_count}</b>", metric_value_style),
        ],
    ]
    
    metrics_table = Table(metrics_data, colWidths=[2.2*inch, 2.2*inch, 2.2*inch])
    metrics_table.setStyle(TableStyle([
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('BOX', (0, 0), (0, -1), 1, truenas_blue),
        ('BOX', (1, 0), (1, -1), 1, truenas_blue),
        ('BOX', (2, 0), (2, -1), 1, truenas_blue),
        ('BACKGROUND', (0, 0), (-1, -1), colors.white),
        ('TOPPADDING', (0, 0), (-1, -1), 12),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
    ]))
    story.append(metrics_table)
    story.append(Spacer(1, 0.8*inch))
    
    # Footer
    story.append(Paragraph("━" * 60, styles['Normal']))
    story.append(Spacer(1, 0.2*inch))
    story.append(Paragraph("<i>Analysis powered by Claude AI</i>", 
                          ParagraphStyle('Footer', parent=styles['Normal'], alignment=TA_CENTER, textColor=colors.grey)))
    story.append(Paragraph("<i>Three-stage hybrid methodology</i>", 
                          ParagraphStyle('Footer2', parent=styles['Normal'], alignment=TA_CENTER, textColor=colors.grey)))
    story.append(Spacer(1, 0.1*inch))
    story.append(Paragraph("<i>Technical details in appendix</i>", 
                          ParagraphStyle('Footer3', parent=styles['Normal'], alignment=TA_CENTER, fontSize=9, textColor=colors.grey)))
    
    story.append(PageBreak())
    
    # ========== SECTION 2: ACCOUNT HEALTH SUMMARY (MOVED UP) ==========
    story.append(Paragraph("Account Health Summary", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    # Calculate account-level metrics
    customer_name = case_analysis[0]['customer_name'] if case_analysis else "Unknown Customer"
    total_cases = len(case_analysis)
    avg_frustration = claude_statistics['avg_frustration_score']
    
    # Calculate holistic health score using strict formula
    health_score, score_breakdown = calculate_account_health_score(case_analysis, claude_statistics)
    
    # Count key risk indicators
    systemic_count = len([c for c in case_analysis if c['claude_analysis']['issue_class'] == 'Systemic'])
    high_frustration_count = claude_statistics['high_frustration']
    challenging_resolutions = len([c for c in case_analysis if c['claude_analysis']['resolution_outlook'] == 'Challenging'])
    critical_cases_count = len([c for c in case_analysis if c['criticality_score'] >= 180])
    
    # Determine overall relationship status with updated thresholds
    if health_score >= 85:
        sentiment_trend = "Healthy - Strong Relationship"
        trend_color = "#16a34a"
        risk_level = "Low Risk"
    elif health_score >= 70:
        sentiment_trend = "Stable - Minor Concerns"
        trend_color = "#fbbf24"
        risk_level = "Low-Medium Risk"
    elif health_score >= 60:
        sentiment_trend = "At Risk - Moderate Concerns"
        trend_color = "#ea580c"
        risk_level = "Medium Risk"
    else:  # < 60
        sentiment_trend = "Critical - High Renewal Risk"
        trend_color = "#DC2626"
        risk_level = "High Risk"
    
    account_summary = f"""
    <b>Customer:</b> {customer_name}<br/>
    <b>Total Cases Analyzed:</b> {total_cases}<br/>
    <b>Average Frustration:</b> {avg_frustration:.2f}/10<br/>
    <b>Account Health Score:</b> <font color="{trend_color}"><b>{health_score:.0f}/100</b></font><br/>
    <br/>
    <b>Health Score Breakdown:</b><br/>
    • Frustration Level: {score_breakdown['frustration_component']:.1f}/30 pts<br/>
    • High Frustration Cases: {score_breakdown['high_frustration_penalty']:.1f}/20 pts<br/>
    • Critical Case Load: {score_breakdown['critical_load_component']:.1f}/20 pts<br/>
    • Systemic Issues: {score_breakdown['systemic_issues_component']:.1f}/15 pts<br/>
    • Resolution Complexity: {score_breakdown['resolution_complexity_component']:.1f}/15 pts<br/>
    <br/>
    """
    
    # Add note if catastrophic override was applied
    if score_breakdown.get('catastrophic_override_applied'):
        override_weight = score_breakdown.get('catastrophic_override_weight', 0)
        catastrophic_count = score_breakdown.get('catastrophic_case_count', 0)
        
        if override_weight == 1.0:
            recency_note = "recent (0-3 months)"
        elif override_weight == 0.5:
            recency_note = "moderately recent (3-6 months)"
        elif override_weight == 0.25:
            recency_note = "older (6-12 months)"
        else:
            recency_note = "historical (>12 months)"
        
        account_summary += f"""
    <font color="#DC2626"><b>⚠ Catastrophic Case Override:</b> {catastrophic_count} case(s) scoring ≥200 detected 
    ({recency_note}). Applied {override_weight*100:.0f}% penalty to High Frustration and Critical Load components.</font><br/>
    <br/>
    """
    
    account_summary += f"""
    <b>Relationship Status:</b> <font color="{trend_color}"><b>{sentiment_trend}</b></font><br/>
    <b>Renewal Risk Level:</b> <font color="{trend_color}"><b>{risk_level}</b></font><br/>
    """
    
    # Add temporal clustering note if detected
    if score_breakdown.get('temporal_clustering_info', {}).get('detected'):
        temporal_info = score_breakdown['temporal_clustering_info']
        penalty_pct = score_breakdown.get('temporal_clustering_penalty', 0) * 100
        base_score = score_breakdown.get('base_health_score', health_score)
        
        account_summary += f"""<br/>
    <font color="#ea580c"><b>⚠️ Temporal Clustering Alert:</b> {temporal_info['description']}. 
    Base score {base_score:.0f} reduced by {penalty_pct:.0f}% due to rapid deterioration pattern.</font><br/>
    """
    
    account_summary += f"""<br/>
    <b>Key Risk Indicators:</b><br/>
    • High Frustration Cases (≥7): {high_frustration_count}<br/>
    • Critical Cases (≥180): {critical_cases_count}<br/>
    • Systemic Issues Identified: {systemic_count}<br/>
    • Challenging Resolutions: {challenging_resolutions}<br/>
    • Cases with Detailed Timelines: {deepseek_statistics.get('total_analyzed', 0)}<br/>
    """
    
    story.append(Paragraph(account_summary, styles['BodyText']))
    story.append(Spacer(1, 0.3*inch))
    
    # SECTION 3: Critical Cases Summary (chronological list)
    cases_with_timelines = [c for c in case_analysis if c.get('deepseek_analysis')]
    
    if len(cases_with_timelines) > 0:
        story.append(Paragraph("Critical Cases Summary (Score ≥180)", styles['SubHeading']))
        story.append(Spacer(1, 0.1*inch))
        
        summary_intro = f"""
        The following {len(cases_with_timelines)} cases exceeded the criticality threshold of 180 points 
        and received detailed timeline analysis. Cases are listed chronologically (oldest to newest) based on 
        first message date to align with the frustration trend chart below, making it easy to correlate case 
        numbers on the chart with their summaries.
        """
        story.append(Paragraph(summary_intro, styles['BodyText']))
        story.append(Spacer(1, 0.15*inch))
        
        # Sort cases chronologically by FIRST MESSAGE DATE (not case creation date)
        def get_first_message_date(case):
            """Get the earliest message date from case data"""
            try:
                case_data = case.get('case_data')
                if case_data is not None and not case_data.empty:
                    # Get all message dates
                    msg_dates = case_data['Message Date'].dropna()
                    if len(msg_dates) > 0:
                        return msg_dates.min()
            except:
                pass
            # Fallback to created_date
            return pd.to_datetime(case['created_date'])
        
        cases_sorted_by_date = sorted(
            cases_with_timelines,
            key=get_first_message_date
        )
        
        for idx, case in enumerate(cases_sorted_by_date, 1):
            deepseek = case.get('deepseek_analysis')
            
            # Build 2-3 sentence summary from DeepSeek analysis
            # Use FIRST MESSAGE DATE for temporal reference
            first_msg_date = get_first_message_date(case)
            date_str = first_msg_date.strftime('%b %Y')  # e.g., "Jun 2024"
            
            case_summary_text = f"<b>{date_str} - Case {case['case_number']}</b> "
            case_summary_text += f"({case['severity']}, {case['case_age_days']} days, "
            case_summary_text += f"{case['claude_analysis']['frustration_score']}/10 frustration, "
            case_summary_text += f"Score: {case['criticality_score']:.0f}): "
            
            # Use DeepSeek's analysis if available, otherwise fall back to Claude
            if deepseek and deepseek.get('pain_points'):
                # Combine pain points and root cause for concise summary
                summary_content = deepseek.get('pain_points', '')
                if deepseek.get('root_cause') and len(summary_content) < 200:
                    summary_content += f" {deepseek.get('root_cause', '')}"
                
                # Truncate if too long (keep to ~2-3 sentences)
                if len(summary_content) > 400:
                    sentences = summary_content.split('. ')
                    summary_content = '. '.join(sentences[:3]) + '.'
                
                case_summary_text += summary_content
            else:
                # Fallback to Claude's key phrase if DeepSeek didn't populate
                key_phrase = case['claude_analysis'].get('key_phrase', '')
                issue_class = case['claude_analysis'].get('issue_class', 'Unknown')
                
                if key_phrase:
                    case_summary_text += f"{issue_class} issue. Customer expressed: \"{key_phrase}\""
                else:
                    case_summary_text += f"{issue_class} issue requiring attention."
            
            story.append(Paragraph(f"{idx}. {case_summary_text}", styles['BodyText']))
            story.append(Spacer(1, 0.12*inch))
        
        story.append(Spacer(1, 0.1*inch))
    else:
        # If no cases met threshold, note it
        story.append(Paragraph("Critical Cases Summary", styles['SubHeading']))
        story.append(Spacer(1, 0.1*inch))
        story.append(Paragraph(
            "<i>No cases exceeded the criticality threshold of 180 points. This indicates a healthy account "
            "with low frustration levels and manageable issues.</i>",
            styles['BodyText']
        ))
        story.append(Spacer(1, 0.2*inch))
    
    # SECTION 4: Frustration Trend Chart
    if 'frustration_trend' in charts:
        story.append(Paragraph("Frustration Trend Over Time", styles['SubHeading']))
        trend_img = Image(io.BytesIO(charts['frustration_trend']))
        trend_img._restrictSize(6*inch, 3.5*inch)
        story.append(trend_img)
        story.append(Spacer(1, 0.2*inch))
        
        trend_explanation = """
        <b>Trend Analysis:</b> This chart shows the customer's average frustration score over time. 
        An upward trend indicates deteriorating relationship health, while a downward trend suggests improving satisfaction. 
        Scores consistently above 7 (red line) indicate high-risk periods requiring immediate attention.
        """
        story.append(Paragraph(trend_explanation, styles['BodyText']))
        story.append(Spacer(1, 0.2*inch))
    
    # SECTION 5: Case Timeline Gantt Chart
    if 'case_volume_trend' in charts:
        story.append(Paragraph("Case Timeline - Recent Activity", styles['SubHeading']))
        gantt_img = Image(io.BytesIO(charts['case_volume_trend']))
        gantt_img._restrictSize(6*inch, 3.5*inch)
        story.append(gantt_img)
        story.append(Spacer(1, 0.15*inch))
        
        gantt_explanation = """
        <b>How to Read This Chart:</b> This Gantt chart shows the 30 most recent cases with their duration and overlap. 
        Each horizontal bar represents one case, starting from the first customer message (left edge) to the last message 
        (right edge). Cases are listed newest-to-oldest from top to bottom, creating a timeline you can read downward. 
        Bar colors indicate priority: <font color="#DC2626"><b>Red (≥190)</b></font> = Critical Priority, 
        <font color="#ea580c"><b>Orange (140-189)</b></font> = High Priority, <font color="#16a34a"><b>Green (&lt;140)</b></font> = 
        Lower Priority. Longer bars indicate cases that remained active over extended periods. Cases marked "(Active)" are 
        still open. Look for clusters of red/orange bars to identify periods of high account stress.
        """
        story.append(Paragraph(gantt_explanation, styles['BodyText']))
        story.append(Spacer(1, 0.2*inch))
    
    story.append(PageBreak())
    
    # SECTION 6: Cases Identified for Deeper Analysis
    story.append(Paragraph("Cases Identified for Deeper Analysis", styles['SectionHeading']))
    story.append(Spacer(1, 0.1*inch))
    
    chart_img = Image(io.BytesIO(charts['top_25_critical']))
    chart_img._restrictSize(6.5*inch, 4*inch)
    story.append(chart_img)
    story.append(Spacer(1, 0.15*inch))
    
    # Add explanation text
    chart_explanation = """
    <b>How to Read This Chart:</b> Cases are ranked by final criticality score (left to right, highest priority at top). 
    Bar colors indicate priority level: <font color="#DC2626"><b>Red (≥190)</b></font> = Critical Priority requiring 
    immediate executive attention, <font color="#ea580c"><b>Orange (140-189)</b></font> = High Priority requiring prompt 
    action, <font color="#16a34a"><b>Green (&lt;140)</b></font> = Lower Priority but still monitored. The criticality 
    score combines AI-detected frustration (0-100pts), technical severity, issue classification (Systemic/Environmental/
    Component/Procedural), resolution complexity, case age, and support level to provide a holistic priority ranking.
    """
    story.append(Paragraph(chart_explanation, styles['BodyText']))
    story.append(Spacer(1, 0.3*inch))
    
    story.append(PageBreak())
    
    # ========== SECTION 7: DETAILED CASE PROFILES ==========
    story.append(Paragraph("Selected Critical Cases - Detailed Profiles", styles['SectionHeading']))
    story.append(Spacer(1, 0.1*inch))
    
    # Get cases that have timelines (hybrid selection: ≥180 OR top 10% outliers)
    cases_with_timelines = [c for c in case_analysis if c.get('deepseek_analysis')]
    profile_count = len(cases_with_timelines)
    
    profile_intro = f"""
    The following profiles provide comprehensive analysis of {profile_count} cases that scored ≥180 on the 
    criticality scale, including Claude 3.5 Sonnet's message-by-message chronological timeline showing customer 
    emotional journey, frustration inflection points, and support response quality assessment.
    """
    story.append(Paragraph(profile_intro, styles['BodyText']))
    story.append(Spacer(1, 0.2*inch))
    
    for i, case in enumerate(cases_with_timelines, 1):
        story.append(Spacer(1, 0.2*inch))
        story.append(Paragraph(f"#{i}: Case {case['case_number']} - {case['customer_name']}", styles['SubHeading']))
        
        claude = case['claude_analysis']
        deepseek = case.get('deepseek_analysis')
        tech_map = case.get('tech_map', {})
        
        case_details = [
            ["Criticality Score", str(round(case['criticality_score'], 1))],
            ["Claude Frustration", f"{claude['frustration_score']}/10 → {case['score_breakdown']['claude_frustration']:.1f}pts"],
            ["Issue Class", f"{claude.get('issue_class', 'Unknown')} → {case['score_breakdown']['issue_class']}pts"],
            ["Resolution Outlook", f"{claude.get('resolution_outlook', 'Unknown')} → {case['score_breakdown']['resolution_outlook']}pts"],
            ["Severity", case['severity']],
            ["Support Level", case.get('support_level', 'Unknown')],
            ["Messages", str(case['interaction_count'])],
            ["Age", f"{case['case_age_days']} days"],
        ]
        
        if deepseek:
            case_details.append(["Claude Sonnet Priority", deepseek.get('customer_priority', 'N/A')])
            if case['score_breakdown'].get('deepseek_timeline', 0) > 0:
                ds_score = case['score_breakdown']['deepseek_timeline']
                frust_rate = case['score_breakdown'].get('deepseek_frustration_rate', 0)
                damage_rate = case['score_breakdown'].get('deepseek_damage_rate', 0)
                case_details.append(["DeepSeek Timeline Bonus", f"+{ds_score:.1f}pts (Frust: {frust_rate:.0f}%, Damage: {damage_rate:.0f}%)"])
        
        case_table = Table(case_details, colWidths=[1.5*inch, 5*inch])
        case_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (0, -1), HexColor("#f0f0f0")),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, -1), 9),
            ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ('TOPPADDING', (0, 0), (-1, -1), 6),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
            ('LEFTPADDING', (0, 0), (-1, -1), 8),
        ]))
        story.append(case_table)
        story.append(Spacer(1, 0.15*inch))
        
        # DeepSeek Analysis
        if deepseek and deepseek.get('analysis_successful'):
            story.append(Paragraph("<b><i>Claude 3.5 Sonnet Deep Analysis:</i></b>", styles['Normal']))
            story.append(Spacer(1, 0.1*inch))
            
            # Track if we have ANY content to display
            has_content = False
            
            if deepseek.get('pain_points'):
                pain_points = replace_tech_emails_with_names(deepseek['pain_points'], tech_map)
                story.append(Paragraph(f"<b>Pain Points:</b> {pain_points}", styles['BodyText']))
                story.append(Spacer(1, 0.08*inch))
                has_content = True
            
            if deepseek.get('root_cause'):
                root_cause = replace_tech_emails_with_names(deepseek['root_cause'], tech_map)
                story.append(Paragraph(f"<b>Root Cause:</b> {root_cause}", styles['BodyText']))
                story.append(Spacer(1, 0.08*inch))
                has_content = True
            
            if deepseek.get('sentiment_trend'):
                sentiment = replace_tech_emails_with_names(deepseek['sentiment_trend'], tech_map)
                story.append(Paragraph(f"<b>Sentiment Trend:</b> {sentiment}", styles['BodyText']))
                story.append(Spacer(1, 0.08*inch))
                has_content = True
            
            if deepseek.get('critical_inflection_points'):
                inflection = replace_tech_emails_with_names(deepseek['critical_inflection_points'], tech_map)
                story.append(Paragraph(f"<b>Critical Inflection Points:</b> {inflection}", styles['BodyText']))
                story.append(Spacer(1, 0.08*inch))
                has_content = True
            
            if deepseek.get('recommended_action'):
                action = replace_tech_emails_with_names(deepseek['recommended_action'], tech_map)
                story.append(Paragraph(f"<b>Recommended Action:</b> {action}", styles['BodyText']))
                has_content = True
            
            # If no executive summary content was extracted, show warning
            if not has_content:
                story.append(Paragraph(
                    '<font color="#DC2626"><b>⚠ Executive Summary Extraction Failed</b></font>',
                    styles['Normal']
                ))
                story.append(Spacer(1, 0.05*inch))
                story.append(Paragraph(
                    'Claude Sonnet analysis completed but executive summary fields could not be parsed. Check parsing logic in Part 2.',
                    styles['BodyText']
                ))
                story.append(Spacer(1, 0.1*inch))
            
            story.append(Spacer(1, 0.2*inch))
            
            # CASE INTERACTION TIMELINE
            timeline_entries = deepseek.get('timeline_entries', [])
            timeline_count = len(timeline_entries)
            
            story.append(Paragraph(
                f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━", 
                styles['Normal']
            ))
            story.append(Paragraph(
                f'<b>📅 CASE INTERACTION TIMELINE ({case["case_age_days"]} days, {case["interaction_count"]} messages)</b>', 
                styles['SubHeading']
            ))
            story.append(Paragraph(
                f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━", 
                styles['Normal']
            ))
            story.append(Spacer(1, 0.15*inch))
            
            if timeline_count > 0:
                for entry in timeline_entries:
                    frustration_detected = entry.get('frustration_detected', '').lower()
                    if 'yes' in frustration_detected:
                        icon = '🔴'
                        label_color = '#DC2626'
                    elif entry.get('customer_tone', '').lower() in ['concerned', 'impatient', 'worried']:
                        icon = '🟡'
                        label_color = '#ea580c'
                    else:
                        icon = '🟢'
                        label_color = '#1e40af'
                    
                    entry_label = entry.get('entry_label', 'Unknown')
                    # Escape HTML in entry label too
                    entry_label_escaped = entry_label.replace('&', '&amp;')
                    entry_label_escaped = entry_label_escaped.replace('<', '&lt;')
                    entry_label_escaped = entry_label_escaped.replace('>', '&gt;')
                    
                    story.append(Paragraph(
                        f'<font color="{label_color}"><b>▼ {icon} {entry_label_escaped}</b></font>', 
                        styles['Normal']
                    ))
                    story.append(Spacer(1, 0.05*inch))
                    
                    if entry.get('summary'):
                        summary = entry['summary']
                        # Replace tech emails with their actual names
                        summary = replace_tech_emails_with_names(summary, tech_map)
                        # Escape HTML entities
                        summary = summary.replace('&', '&amp;')
                        summary = summary.replace('<', '&lt;')
                        summary = summary.replace('>', '&gt;')
                        story.append(Paragraph(f'   <b>Summary:</b> {summary}', styles['BodyText']))
                        story.append(Spacer(1, 0.03*inch))
                    
                    if entry.get('customer_tone'):
                        customer_tone = entry['customer_tone'].replace('&', '&amp;')
                        customer_tone = customer_tone.replace('<', '&lt;')
                        customer_tone = customer_tone.replace('>', '&gt;')
                        story.append(Paragraph(
                            f'   <i>Customer Tone:</i> {customer_tone}', 
                            styles['Normal']
                        ))
                        story.append(Spacer(1, 0.03*inch))
                    
                    # Display actual message excerpt if available (with highlighted frustration)
                    # HTML is already escaped in Part 1, with intentional formatting tags added
                    if entry.get('message_excerpt'):
                        story.append(Paragraph(
                            f'   <b>Message Excerpt:</b>', 
                            styles['Normal']
                        ))
                        try:
                            story.append(Paragraph(
                                f'   <i>{entry["message_excerpt"]}</i>', 
                                styles['BodyText']
                            ))
                        except Exception as e:
                            # If excerpt fails to render, show error instead of crashing
                            story.append(Paragraph(
                                f'   <i>[Excerpt rendering failed]</i>', 
                                styles['BodyText']
                            ))
                        story.append(Spacer(1, 0.05*inch))
                    
                    # Show AI's frustration analysis only if frustrated
                    if 'yes' in frustration_detected and entry.get('frustration_detail'):
                        frust_detail = entry['frustration_detail']
                        # Replace tech emails with their actual names
                        frust_detail = replace_tech_emails_with_names(frust_detail, tech_map)
                        # Escape HTML entities
                        frust_detail = frust_detail.replace('&', '&amp;')
                        frust_detail = frust_detail.replace('<', '&lt;')
                        frust_detail = frust_detail.replace('>', '&gt;')
                        story.append(Paragraph(
                            f'   <font color="#DC2626"><b>⚠ Analysis:</b> {frust_detail}</font>', 
                            styles['BodyText']
                        ))
                        story.append(Spacer(1, 0.03*inch))
                    
                    # Display positive message excerpt if available
                    if entry.get('positive_excerpt'):
                        story.append(Paragraph(
                            f'   <b>Positive Response:</b>', 
                            styles['Normal']
                        ))
                        try:
                            story.append(Paragraph(
                                f'   <i>{entry["positive_excerpt"]}</i>', 
                                styles['BodyText']
                            ))
                        except Exception as e:
                            story.append(Paragraph(
                                f'   <i>[Excerpt rendering failed]</i>', 
                                styles['BodyText']
                            ))
                        story.append(Spacer(1, 0.05*inch))
                    
                    # NEW: Check for failure pattern FIRST (surgical addition)
                    failure_pattern = entry.get('failure_pattern_detected', '').lower()
                    if 'yes' in failure_pattern and entry.get('failure_pattern_detail'):
                        # Display failure pattern INSTEAD of positive action
                        pattern_detail = entry['failure_pattern_detail']
                        pattern_detail = replace_tech_emails_with_names(pattern_detail, tech_map)
                        pattern_detail = pattern_detail.replace('&', '&amp;')
                        pattern_detail = pattern_detail.replace('<', '&lt;')
                        pattern_detail = pattern_detail.replace('>', '&gt;')
                        story.append(Paragraph(
                            f'   <font color="#DC2626"><b>⚠️ Failure Pattern:</b> {pattern_detail}</font>', 
                            styles['BodyText']
                        ))
                        story.append(Spacer(1, 0.03*inch))
                    else:
                        # Show positive action ONLY if no failure pattern detected
                        positive_action = entry.get('positive_action_detected', '').lower()
                        if 'yes' in positive_action and entry.get('positive_action_detail'):
                            positive_detail = entry['positive_action_detail']
                            # Replace tech emails with their actual names
                            positive_detail = replace_tech_emails_with_names(positive_detail, tech_map)
                            # Escape HTML entities
                            positive_detail = positive_detail.replace('&', '&amp;')
                            positive_detail = positive_detail.replace('<', '&lt;')
                            positive_detail = positive_detail.replace('>', '&gt;')
                            story.append(Paragraph(
                                f'   <font color="#16a34a"><b>✓ Positive Action:</b> {positive_detail}</font>', 
                                styles['BodyText']
                            ))
                            story.append(Spacer(1, 0.03*inch))
                    
                    story.append(Spacer(1, 0.15*inch))
                
                story.append(Paragraph(
                    f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━", 
                    styles['Normal']
                ))
                story.append(Spacer(1, 0.1*inch))
            else:
                story.append(Paragraph(
                    '<font color="#DC2626"><b>⚠ Timeline Generation Failed</b></font>', 
                    styles['Normal']
                ))
                story.append(Spacer(1, 0.05*inch))
                story.append(Paragraph(
                    f'Claude Sonnet returned 0 timeline entries for {case["interaction_count"]} messages. Check parsing logic or response format.',
                    styles['BodyText']
                ))
                if deepseek.get('raw_response'):
                    preview = deepseek['raw_response'][:400].replace('<', '&lt;').replace('>', '&gt;')
                    story.append(Paragraph(
                        f'<i>Response preview:</i><br/><font size="7">{preview}...</font>',
                        styles['BodyText']
                    ))
                story.append(Spacer(1, 0.1*inch))
                story.append(Paragraph(
                    f"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━", 
                    styles['Normal']
                ))
                story.append(Spacer(1, 0.1*inch))
                
        else:
            story.append(Paragraph("<i>Claude Quick Analysis:</i>", styles['Normal']))
            if claude.get('key_phrase'):
                story.append(Paragraph(f"<b>Key Phrase:</b> \"{claude['key_phrase']}\"", styles['BodyText']))
        
        if i % 2 == 0 and i < profile_count:
            story.append(PageBreak())
    
    story.append(PageBreak())
    
    # ========== SECTION 8: ASSET CORRELATION ANALYSIS ==========
    if asset_correlations and asset_correlations.get('serials_with_multiple_cases', 0) > 0:
        story.append(Paragraph("Asset Correlation Analysis", styles['SectionHeading']))
        story.append(Spacer(1, 0.15*inch))
        
        disclaimer = """
        <i>Note: This analysis provides best-effort correlation of serial numbers found in case data and messages. 
        Asset tracking may be incomplete. Use these insights as investigation starting points, not definitive conclusions.</i>
        """
        story.append(Paragraph(disclaimer, styles['BodyText']))
        story.append(Spacer(1, 0.2*inch))
        
        # Components appearing in multiple cases
        recurring = asset_correlations.get('recurring_serials', [])
        if recurring:
            story.append(Paragraph("Components Appearing in Multiple Cases:", styles['SubHeading']))
            story.append(Spacer(1, 0.1*inch))
            
            for idx, asset in enumerate(recurring[:10], 1):  # Show top 10
                serial = asset['serial']
                case_count = asset['case_count']
                avg_score = asset['avg_criticality']
                component_type = asset['component_type']
                is_refurb = asset['is_refurb']
                
                # Determine risk level
                if case_count >= 3:
                    risk_level = "HIGH RISK"
                    risk_color = "#DC2626"
                elif case_count == 2 and avg_score > 150:
                    risk_level = "MEDIUM RISK"
                    risk_color = "#ea580c"
                else:
                    risk_level = "Monitor"
                    risk_color = "#fbbf24"
                
                # Build case list
                case_numbers = [str(c['case_number']) for c in asset['cases']]
                case_list = ', '.join(case_numbers)
                
                refurb_note = f" ({asset['refurb_level']} - Refurbished)" if is_refurb else ""
                
                asset_text = f"""
                <b>{idx}. {serial}</b> ({component_type}{refurb_note}) - {case_count} cases<br/>
                   • Cases: {case_list}<br/>
                   • Average Criticality: {avg_score:.0f}<br/>
                   • <font color="{risk_color}"><b>Risk Assessment: {risk_level}</b></font><br/>
                """
                
                # Add recommendation based on pattern
                if case_count >= 3:
                    if component_type == 'Chassis':
                        recommendation = "Recurring issues on same chassis may indicate environmental, deployment, or hardware problem. Consider full system review or replacement."
                    else:
                        recommendation = "Component appearing in 3+ cases may be defective or experiencing recurring failure. Consider proactive replacement."
                    
                    if is_refurb:
                        recommendation += " Note: Refurbished component - consider deploying new (A1) component instead."
                    
                    asset_text += f"   • Recommendation: {recommendation}<br/>"
                
                story.append(Paragraph(asset_text, styles['BodyText']))
                story.append(Spacer(1, 0.15*inch))
        
        # Refurb component usage
        refurb_count = asset_correlations.get('refurb_case_count', 0)
        if refurb_count > 0:
            story.append(Spacer(1, 0.2*inch))
            story.append(Paragraph("━" * 60, styles['Normal']))
            story.append(Spacer(1, 0.15*inch))
            story.append(Paragraph("Refurbished Component Usage:", styles['SubHeading']))
            story.append(Spacer(1, 0.1*inch))
            
            refurb_pct = (refurb_count / asset_correlations['total_cases'] * 100)
            breakdown = asset_correlations.get('refurb_breakdown', {})
            
            refurb_text = f"""
            <font color="#ea580c"><b>⚠️ {refurb_count} of {asset_correlations['total_cases']} cases ({refurb_pct:.0f}%) involve refurbished components:</b></font><br/>
            • R1 components (refurbed once): {breakdown.get('R1', 0)} instances<br/>
            • R2 components (refurbed twice): {breakdown.get('R2', 0)} instances<br/>
            • R3 components (refurbed 3+ times): {breakdown.get('R3', 0)} instances<br/>
            <br/>
            <i>Note: R2/R3 components have higher failure risk. Consider prioritizing new (A1) components for critical accounts.</i>
            """
            story.append(Paragraph(refurb_text, styles['BodyText']))
        
        # Coverage stats
        story.append(Spacer(1, 0.2*inch))
        story.append(Paragraph("━" * 60, styles['Normal']))
        story.append(Spacer(1, 0.15*inch))
        story.append(Paragraph("Asset Tracking Coverage:", styles['SubHeading']))
        story.append(Spacer(1, 0.1*inch))
        
        coverage_text = f"""
        • Total unique serials found: {asset_correlations['total_serials_tracked']}<br/>
        • Serials appearing in 2+ cases: {asset_correlations['serials_with_multiple_cases']}<br/>
        • Cases with asset data: {asset_correlations['cases_with_asset_data']} of {asset_correlations['total_cases']} ({asset_correlations['coverage_percent']:.0f}%)<br/>
        <br/>
        <i>Asset correlation is best-effort based on available serial data from both the Asset Serial field and message content. 
        Some cases may have incomplete asset information. This analysis helps identify potential hardware/deployment patterns 
        but should be validated with asset management records.</i>
        """
        story.append(Paragraph(coverage_text, styles['BodyText']))
    
    story.append(PageBreak())
    
    # ========== APPENDIX - TECHNICAL DETAILS ==========
    story.append(Spacer(1, 0.5*inch))
    story.append(Paragraph("━" * 80, styles['Normal']))
    story.append(Paragraph("APPENDIX - Technical Details", styles['ReportTitle']))
    story.append(Paragraph("━" * 80, styles['Normal']))
    story.append(Spacer(1, 0.3*inch))
    
    appendix_intro = """
    The following sections provide technical details about the analysis methodology, scoring framework, 
    and supporting visualizations. These are included for transparency and technical review but are not 
    required for understanding the main findings.
    """
    story.append(Paragraph(appendix_intro, styles['BodyText']))
    story.append(PageBreak())
    
    # APPENDIX A: Executive Summary (Methodology)
    story.append(Paragraph("Appendix A: Executive Summary - Methodology", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    exec_text = f"""
    <b>Hybrid Analysis Methodology:</b> This report uses a three-stage hybrid approach combining Claude 3.5 Haiku
    for rapid analysis of all {len(case_analysis)} cases, followed by Claude 3.5 Sonnet quick scoring of the top
    {deepseek_statistics['total_scored']} cases, and finally Claude 3.5 Sonnet deep-dive analysis of the top
    {deepseek_statistics['total_analyzed']} critical cases with detailed interaction timelines.
    <br/><br/>
    <b>Context-Aware Analysis:</b> The AI models were provided with comprehensive TrueNAS context including 
    company information, product lines (F/M/H/R-Series), severity level definitions (S1-S4), SLA response times, 
    support tier details (Gold/Silver/Bronze), and enterprise customer expectations.
    <br/><br/>
    <b>Stage 1 - Claude 3.5 Haiku:</b> Analyzed all cases in {claude_time:.1f} seconds, providing frustration
    scores (0-10), issue classification, and resolution outlook assessment. Identified {claude_statistics['high_frustration']}
    high-frustration cases.
    <br/><br/>
    <b>Stage 2A - Claude 3.5 Sonnet Quick Scoring:</b> Conducted pattern analysis of top {deepseek_statistics['total_scored']}
    cases in {deepseek_statistics.get('quick_scoring_time', 0):.1f} seconds, assessing frustration frequency and relationship
    damage rates to refine prioritization.
    <br/><br/>
    <b>Stage 2B - Claude 3.5 Sonnet Detailed Timelines:</b> Generated comprehensive analysis of {deepseek_statistics['total_analyzed']}
    cases using hybrid selection (absolute ≥180 OR relative outliers in top 10%) with minimum 3 and maximum 10 timelines, 
    completed in {deepseek_statistics.get('detailed_timeline_time', 0):.1f} seconds. Creates message-by-message chronological 
    timelines that map customer emotional journey, identify frustration inflection points, detect failure patterns (repeated 
    escalations/failed remediations), and assess support response quality with ownership attribution (customer vs. support delays).
    """
    story.append(Paragraph(exec_text, styles['BodyText']))
    story.append(PageBreak())
    
    # APPENDIX B: Scoring Framework
    story.append(Paragraph("Appendix B: Hybrid Scoring Framework", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    scoring_data = [
        ["Factor", "Weight", "Points Range", "Analysis Stage"],
        ["Claude Frustration", "PRIMARY", "0-100 pts (log curve)", "Stage 1"],
        ["Technical Severity", "Context", "5-35 pts", "Data"],
        ["Message Volume", "Context", "5-30 pts (inverted)", "Data"],
        ["Issue Class", "Impact", "5-30 pts", "Stage 1"],
        ["Resolution Outlook", "Complexity", "0-15 pts", "Stage 1"],
        ["Support Level", "Priority", "0-10 pts", "Data"],
        ["Engagement", "Minimal", "0-15 pts", "Data"],
        ["Case Age", "Minimal", "0-10 pts", "Data"],
    ]
    
    scoring_table = Table(scoring_data, colWidths=[1.5*inch, 1*inch, 1.8*inch, 1.3*inch])
    scoring_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), HexColor("#1e40af")),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('FONTSIZE', (0, 1), (-1, -1), 9),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 8),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, HexColor("#f9fafb")]),
    ]))
    story.append(scoring_table)
    story.append(PageBreak())
    
    # APPENDIX C: Analysis Visualizations
    story.append(Paragraph("Appendix C: Analysis Visualizations", styles['SectionHeading']))
    story.append(Spacer(1, 0.2*inch))
    
    story.append(Paragraph("Frustration Distribution", styles['SubHeading']))
    frust_img = Image(io.BytesIO(charts['frustration_distribution']))
    frust_img._restrictSize(5*inch, 3.5*inch)
    story.append(frust_img)
    story.append(Spacer(1, 0.3*inch))
    
    story.append(Paragraph("Score Breakdown - Top 10", styles['SubHeading']))
    score_img = Image(io.BytesIO(charts['score_breakdown']))
    score_img._restrictSize(6.5*inch, 4*inch)
    story.append(score_img)
    
    story.append(PageBreak())
    
    # APPENDIX D: Support Level Distribution
    story.append(Paragraph("Appendix D: Support Level Distribution", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    support_img = Image(io.BytesIO(charts['support_level_distribution']))
    support_img._restrictSize(5*inch, 3.5*inch)
    story.append(support_img)
    story.append(Spacer(1, 0.2*inch))
    
    support_text = """
    <b>Support Level Context:</b> The distribution of support levels affects case prioritization and SLA expectations.
    Gold customers receive 24x7 support with 2-hour response for S1 issues, Silver receives 24x5 support, and Bronze 
    receives 12x5 business hours support. Gold customers receive a +10 point priority bonus, Silver receives +5 points 
    in the criticality scoring model.
    """
    story.append(Paragraph(support_text, styles['BodyText']))
    story.append(PageBreak())
    
    # APPENDIX E: Code Version Information
    story.append(Paragraph("Appendix E: Code Version Information", styles['SectionHeading']))
    version_data = [[k, v] for k, v in code_versions.items()]
    version_table = Table(version_data, colWidths=[2*inch, 4*inch])
    version_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (0, -1), HexColor("#e0e0e0")),
        ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),
        ('FONTNAME', (1, 0), (1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 9),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 5),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 5),
        ('LEFTPADDING', (0, 0), (-1, -1), 8),
    ]))
    story.append(version_table)
    story.append(PageBreak())
    
    # APPENDIX F: Key Findings
    story.append(Paragraph("Appendix F: Key Findings", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    findings = [
        f"<b>Context-Aware Analysis:</b> AI models provided with TrueNAS-specific context for accurate assessment.",
        f"<b>Support Level Integration:</b> Gold customers receive +10 priority points, Silver +5 points.",
        f"<b>Rapid Coverage:</b> Claude 3.5 Haiku analyzed {claude_statistics['total_analyzed']} cases in {claude_time:.1f}s.",
        f"<b>Smart Prioritization:</b> Claude 3.5 Sonnet quick-scored {deepseek_statistics.get('total_scored', 0)} cases to identify top priorities.",
        f"<b>Deep Interaction Timelines:</b> Claude 3.5 Sonnet generated message-by-message analysis for {deepseek_statistics['total_analyzed']} cases.",
        f"<b>Top Case:</b> Case #{top_25_critical[0]['case_number']} scores {top_25_critical[0]['criticality_score']:.0f} points with {top_25_critical[0]['claude_analysis']['frustration_score']}/10 frustration.",
    ]
    
    for finding in findings:
        story.append(Paragraph(f"• {finding}", styles['BodyText']))
        story.append(Spacer(1, 0.1*inch))
    
    story.append(PageBreak())
    
    # APPENDIX G: Recommendations
    story.append(Paragraph("Appendix G: Recommendations", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    # Recalculate metrics for recommendations (in case not in scope)
    critical_cases_count = len([c for c in case_analysis if c['criticality_score'] >= 180])
    systemic_count = len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Systemic'])
    challenging_resolutions = len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Challenging'])
    
    recommendations = [
        ("<b>1. Prioritize Critical Cases</b>",
         f"{critical_cases_count} cases scored ≥180 requiring immediate attention and executive engagement."),
        ("<b>2. Act on Timeline Analysis</b>",
         f"{deepseek_statistics['total_analyzed']} cases received detailed timelines with specific action plans. Implement immediately."),
        ("<b>3. Address Systemic Issues</b>",
         f"{systemic_count} systemic issues identified indicate broader product or process problems requiring strategic attention."),
        ("<b>4. Monitor Challenging Resolutions</b>",
         f"{challenging_resolutions} cases with challenging resolution outlooks may require escalation or architectural changes."),
        ("<b>5. Review Response Ownership Patterns</b>",
         "Timeline analysis distinguishes customer vs. support delays - ensure we're meeting SLA commitments."),
    ]
    
    for title, desc in recommendations:
        story.append(Spacer(1, 0.15*inch))
        story.append(Paragraph(title, styles['Normal']))
        story.append(Spacer(1, 0.05*inch))
        story.append(Paragraph(desc, styles['BodyText']))
    
    story.append(PageBreak())
    
    # APPENDIX I: How Scoring Works (Detailed Explanation)
    story.append(Paragraph("Appendix I: How Account Health Scoring Works", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    scoring_explanation = """
    <b>Overview:</b> The Account Health Score (0-100) combines five weighted components to assess relationship 
    health. Higher scores indicate healthier accounts. The system applies intelligent overrides and penalties 
    to detect patterns that numeric averages might miss.
    <br/><br/>
    <b>BASE SCORE COMPONENTS (Maximum 100 Points):</b>
    <br/><br/>
    <b>1. Average Frustration Level (0-30 points):</b><br/>
    Measures overall customer sentiment across all cases.<br/>
    • Formula: 30 - (average_frustration × 3)<br/>
    • Example: Avg frustration 2/10 → 30 - 6 = 24 points<br/>
    • Example: Avg frustration 8/10 → 30 - 24 = 6 points<br/>
    <br/>
    <b>2. High Frustration Cases (0-20 points):</b><br/>
    Penalty for cases with frustration ≥7/10.<br/>
    • Formula: 20 - (high_frustration_ratio × 100)<br/>
    • Example: 0% high frustration → 20 points (perfect)<br/>
    • Example: 10% high frustration → 10 points (half penalty)<br/>
    • Example: 20%+ high frustration → 0 points (full penalty)<br/>
    <br/>
    <b>3. Critical Case Load (0-20 points):</b><br/>
    Penalty for cases scoring ≥180 on criticality scale.<br/>
    • Formula: 20 - (critical_ratio × 100)<br/>
    • Example: 0% critical cases → 20 points<br/>
    • Example: 5% critical cases → 15 points<br/>
    • Example: 20%+ critical → 0 points<br/>
    <br/>
    <b>4. Systemic Issues (0-15 points):</b><br/>
    Penalty for systemic problems (product/architecture issues vs. one-off component failures).<br/>
    • Formula: 15 - (systemic_ratio × 75)<br/>
    • Example: 0% systemic → 15 points<br/>
    • Example: 10% systemic → 7.5 points<br/>
    <br/>
    <b>5. Resolution Complexity (0-15 points):</b><br/>
    Penalty for cases with challenging resolution outlook.<br/>
    • Formula: 15 - (challenging_ratio × 75)<br/>
    • Example: 0% challenging → 15 points<br/>
    • Example: 20% challenging → 0 points<br/>
    <br/><br/>
    <b>INTELLIGENT OVERRIDES & PENALTIES:</b>
    <br/><br/>
    <b>Catastrophic Case Override (Recency-Based):</b><br/>
    If ANY case scores ≥200, apply graduated penalty based on how recent:<br/>
    • 0-3 months old: 100% override (forces High Frustration and Critical Load to 0 points)<br/>
    • 3-6 months old: 50% override (half penalty)<br/>
    • 6-12 months old: 25% override (quarter penalty)<br/>
    • 12+ months old: 0% override (no penalty - account recovered)<br/>
    <br/>
    <i>Rationale: One catastrophic case tanks the score if recent, but old disasters shouldn't dominate 
    if account is healthy now.</i>
    <br/><br/>
    <b>Temporal Clustering Penalty:</b><br/>
    Detects rapid deterioration by identifying multiple concerning cases (≥140 & top 20%) in short timeframes:<br/>
    • 1 case in 60 days: 10% penalty to final score<br/>
    • 2 cases within 14 days: 40% penalty (rapid deterioration!)<br/>
    • 2 cases within 30 days: 25% penalty (concerning trend)<br/>
    • 3+ cases within 14 days: 70% penalty (severe deterioration!)<br/>
    • 3+ cases within 30 days: 50% penalty (major concern)<br/>
    <br/>
    <i>Rationale: Multiple problems happening simultaneously indicates systemic breakdown, not isolated incidents.</i>
    <br/><br/>
    <b>TIMELINE SELECTION (Hybrid Logic):</b>
    <br/><br/>
    Cases receive detailed timeline analysis if they meet EITHER condition:<br/>
    • <b>Absolute Threshold:</b> Score ≥180 (always concerning regardless of account baseline)<br/>
    • <b>Relative Threshold:</b> Score ≥140 AND in top 10% for this account (outlier detection)<br/>
    <br/>
    Plus enforcement rules:<br/>
    • Minimum: 3 timelines (ensures context even for healthy accounts)<br/>
    • Maximum: 10 timelines (prevents analysis overload)<br/>
    <br/>
    <i>Rationale: Healthy accounts need to catch their outliers (e.g., score 165 is bad for 90-avg account). 
    Unhealthy accounts need to focus only on truly critical cases (e.g., score 165 is normal for 150-avg account).</i>
    <br/><br/>
    <b>RISK LEVEL THRESHOLDS:</b><br/>
    • ≥85: Healthy - Low Risk<br/>
    • ≥70: Stable - Low-Medium Risk<br/>
    • ≥60: At Risk - Medium Risk<br/>
    • &lt;60: Critical - High Renewal Risk<br/>
    """
    
    story.append(Paragraph(scoring_explanation, styles['BodyText']))
    story.append(Spacer(1, 0.3*inch))
    
    # Add example scenarios
    story.append(Paragraph("Example Scenarios:", styles['SubHeading']))
    story.append(Spacer(1, 0.1*inch))
    
    examples = """
    <b>Example 1: Healthy Account with Recent Spike</b><br/>
    49 cases, avg frustration 1.4, but 2 cases scored 171 & 173 in last 2 weeks.<br/>
    • Base score: 91/100<br/>
    • Temporal clustering: 40% penalty (2 cases within 14 days)<br/>
    • Final score: 91 × 0.6 = 55/100 → "Critical - High Renewal Risk"<br/>
    • Timelines: 3 cases (171, 173 caught via relative threshold + 1 absolute)<br/>
    <br/>
    <b>Example 2: Account Recovered from Past Crisis</b><br/>
    Case scored 250 eighteen months ago, but all recent cases &lt;100.<br/>
    • Catastrophic override: 0% (>12 months old)<br/>
    • Temporal clustering: 0% (no recent concerning cases)<br/>
    • Final score: 92/100 → "Healthy - Low Risk"<br/>
    <br/>
    <b>Example 3: Chronic Issues (Not Accelerating)</b><br/>
    Account consistently gets 150-170 cases spread over months.<br/>
    • Base score: 65/100<br/>
    • Temporal clustering: 10% penalty (1 recent concerning case)<br/>
    • Final score: 65 × 0.9 = 58.5/100 → "Critical - High Renewal Risk"<br/>
    • Timelines: Only absolute threshold cases (≥180), relative threshold doesn't trigger much<br/>
    """
    
    story.append(Paragraph(examples, styles['BodyText']))
    
    story.append(PageBreak())
    
    # APPENDIX J: Conclusion
    story.append(Paragraph("Appendix J: Conclusion", styles['SectionHeading']))
    story.append(Spacer(1, 0.15*inch))
    
    conclusion = f"""
    This three-stage analysis successfully combines rapid Claude 3.5 Haiku coverage with efficient Claude 3.5 Sonnet
    prioritization and deep timeline insights, analyzing {len(case_analysis)} cases in {total_time:.1f} seconds. 
    The methodology balances cost-efficiency with analytical depth, ensuring every case is evaluated while providing 
    detailed insights where they matter most.
    <br/><br/>
    <b>Comprehensive Interaction Timelines:</b> For each critical case, Claude 3.5 Sonnet generated detailed 
    chronological timelines analyzing every message for emotional signals, frustration triggers, and support 
    response quality - providing unprecedented visibility into how customer relationships evolve.
    """
    story.append(Paragraph(conclusion, styles['BodyText']))
    story.append(Spacer(1, 0.5*inch))
    
    footer = f"<i>Generated: {current_date.strftime('%B %d, %Y at %I:%M %p')}</i>"
    story.append(Paragraph(footer, styles['Normal']))
    
    doc.build(story)
    return pdf_buffer.getvalue()
def customer_sentiment_analysis(excel_file, analysis_context=None):
    """
    Account-Level Health Assessment
    Stage 1: Claude 3.5 Haiku analyzes ALL cases for this customer
    Stage 2A: Claude 3.5 Sonnet quick scoring on top 25 cases
    Stage 2B: Claude 3.5 Sonnet detailed timelines (hybrid: ≥180 OR top 10% outliers)
    
    MAIN ENTRY POINT - Abacus AI Workflow
    """
    
    client = ApiClient()
    client.stream_message("="*70 + "\n")
    client.stream_message("ACCOUNT HEALTH ASSESSMENT\n")
    client.stream_message("="*70 + "\n")
    client.stream_message("Stage 1: Claude 3.5 Haiku - All cases for this customer\n")
    client.stream_message("Stage 2A: Claude 3.5 Sonnet - Pattern scoring (top 25)\n")
    client.stream_message("Stage 2B: Claude 3.5 Sonnet - Timelines (score ≥180)\n")
    client.stream_message("="*70 + "\n\n")
    
    if analysis_context is None:
        analysis_context = """
COMPANY & PRODUCT CONTEXT:
TrueNAS is an enterprise open-source storage company serving Fortune 500 customers globally. 
Products: TrueNAS F-Series (high-performance NVMe), M-Series (high-capacity all-flash/hybrid), 
H-Series (versatile & power-efficient), R-Series (single controller appliance).
Technology: ZFS file system, self-healing architecture, unified storage for virtualization and backup.

SUPPORT TIER CONTEXT:
- Gold Support: 24x7 for S1/S2, 4-hour on-site response, proactive monitoring
- Silver Support: 24x5 for S1/S2, next business day on-site response
- Bronze Support: 12x5 business hours, email support, next business day parts

SEVERITY LEVEL DEFINITIONS:
- S1: System not serving data OR severe performance degradation critically disrupting business operations
  → CRITICAL: Production down, data inaccessible, business impact
  → SLA: 2-hour response, 24x7 (Gold/Silver)
  
- S2: Performance degradation in production OR intermittent faults affecting operations
  → HIGH: System functional but degraded, impacting productivity
  → SLA: 4-hour response, 24x7 (Gold), 24x5 (Silver)
  
- S3: Issue or defect causing minimal business impact
  → MEDIUM: Minor problems, workarounds available
  → SLA: 4-hour email response during business hours
  
- S4: Information requests or administrative questions
  → LOW: General inquiries, how-to questions
  → SLA: Next business day response

CUSTOMER PROFILE:
Enterprise B2B customers with mission-critical storage needs. Customers expect:
- Fast response for production issues (S1/S2)
- Expert technical knowledge of ZFS, storage, and enterprise infrastructure
- Professional communication with minimal back-and-forth
- Clear escalation paths and regular updates
- Hardware replacement within SLA commitments

COMMON ISSUES TO RECOGNIZE:
- Storage performance problems (IOPS, latency, throughput)
- Data integrity concerns (scrub errors, disk failures)
- Replication/backup issues affecting disaster recovery
- Hardware failures (drives, controllers, power supplies)
- Software upgrade complications
- Network connectivity or configuration issues

CHURN RISK INDICATORS:
- Threats to switch to competitors (NetApp, Dell EMC, Pure Storage)
- Mentions of contract renewal concerns
- Executive escalation requests
- Repeated issues without resolution
- SLA violations or missed commitments
- Loss of trust in product or support team
"""
    
    client.stream_message("Using TrueNAS-specific context for AI analysis\n\n")
    
    start_time = time.time()
    
    try:
        # STAGE 1: Load data
        df, current_date = load_and_prepare_data(excel_file, client)
        
        # STAGE 1.5: Detect and merge duplicate cases (NEW - surgical addition)
        df = detect_and_merge_case_relationships(df, client)
        
        # STAGE 2: Claude analysis
        case_analysis, claude_statistics, issue_categories, support_level_distribution, claude_time = \
            run_claude_analysis(df, analysis_context, client)
        
        # STAGE 3: Criticality scoring
        case_analysis = calculate_criticality_scores(case_analysis, client)
        
        # Build lightweight account brief for quick scoring (before asset analysis)
        account_brief_light = build_account_intelligence_brief(case_analysis, asset_correlations=None, mode='light')
        
        # STAGE 2A: Claude Sonnet quick scoring (top 25) with account context
        deepseek_quick_statistics, deepseek_quick_time = run_deepseek_quick_scoring(
            case_analysis, analysis_context, client, account_brief_light
        )
        
        # STAGE 5.5: Asset correlation analysis (needed for detailed brief)
        asset_correlations = analyze_asset_correlations(case_analysis, client)
        
        # Build full account brief with asset intelligence for timelines
        account_brief_full = build_account_intelligence_brief(case_analysis, asset_correlations, mode='full')
        
        # STAGE 2B: Claude Sonnet detailed timelines (hybrid selection) with full context
        deepseek_detailed_statistics, deepseek_detailed_time = run_deepseek_detailed_timeline(
            case_analysis, analysis_context, client, account_brief_full, asset_correlations
        )
        
        # Combine Claude Sonnet statistics
        deepseek_statistics = {
            "total_scored": deepseek_quick_statistics['total_scored'],
            "total_analyzed": deepseek_detailed_statistics['total_analyzed'],
            "api_errors": deepseek_quick_statistics['api_errors'] + deepseek_detailed_statistics['api_errors'],
            "quick_scoring_time": deepseek_quick_time,
            "detailed_timeline_time": deepseek_detailed_time,
            "analysis_time_seconds": deepseek_quick_time + deepseek_detailed_time,
        }
        deepseek_time = deepseek_quick_time + deepseek_detailed_time
        
        total_time = time.time() - start_time
        
        client.stream_message("\n" + "="*70 + "\n")
        client.stream_message(f"ANALYSIS COMPLETE: {total_time:.1f}s total\n")
        client.stream_message(f"  Claude Haiku: {claude_statistics['total_analyzed']} cases in {claude_time:.1f}s\n")
        client.stream_message(f"  Claude Sonnet Pattern Scoring: {deepseek_statistics['total_scored']} cases in {deepseek_quick_time:.1f}s\n")
        client.stream_message(f"  Claude Sonnet Deep Analysis: {deepseek_statistics['total_analyzed']} cases in {deepseek_detailed_time:.1f}s\n")
        client.stream_message(f"  Total Claude Sonnet: {deepseek_time:.1f}s\n")
        client.stream_message("="*70 + "\n\n")
        
        # STAGE 5: Visualizations
        client.stream_message("Generating visualizations...\n")
        
        severity_distribution = {}
        for case in case_analysis:
            sev = case["severity"]
            severity_distribution[sev] = severity_distribution.get(sev, 0) + 1
        
        charts = generate_all_charts(
            case_analysis, 
            claude_statistics, 
            issue_categories, 
            severity_distribution,
            support_level_distribution
        )
        
        # STAGE 6: PDF report
        client.stream_message("Generating PDF report...\n")
        
        pdf_bytes = generate_pdf_report(
            case_analysis,
            claude_statistics,
            deepseek_statistics,
            claude_time,
            deepseek_time,
            total_time,
            current_date,
            charts,
            asset_correlations  # NEW: Pass asset data
        )
        
        client.stream_message(f"✓ PDF: {len(pdf_bytes)} bytes\n")
        
        # STAGE 7: JSON outputs
        client.stream_message("Preparing JSON outputs...\n\n")
        
        def clean_for_json(case_list):
            cleaned = []
            for case in case_list:
                case_copy = case.copy()
                case_copy.pop('case_data', None)
                case_copy.pop('messages_full', None)
                cleaned.append(case_copy)
            return cleaned
        
        top_25_critical = case_analysis[:25]
        
        # NEW: Calculate account-level summary metrics
        customer_name = case_analysis[0]['customer_name'] if case_analysis else "Unknown Customer"
        
        # Use holistic health score calculation (defined in Part 4)
        account_health_score, score_breakdown = calculate_account_health_score(case_analysis, claude_statistics)
        
        # Count issue classes and resolutions
        systemic_issues = len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Systemic'])
        environmental_issues = len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Environmental'])
        challenging_resolutions = len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Challenging'])
        
        critical_cases_analysis = {
            "analysis_date": current_date.strftime("%Y-%m-%d"),
            "methodology": "Hybrid: Claude 3.5 Haiku + Claude 3.5 Sonnet",
            "account_name": customer_name,  # NEW
            "account_health_score": round(account_health_score, 1),  # NEW
            "total_cases_analyzed": len(case_analysis),
            "claude_statistics": claude_statistics,
            "deepseek_statistics": deepseek_statistics,
            "top_critical_cases": clean_for_json(top_25_critical),
        }
        
        summary_statistics = {
            "analysis_date": current_date.strftime("%Y-%m-%d"),
            "account_name": customer_name,  # NEW
            "account_health_score": round(account_health_score, 1),  # NEW
            "total_cases": len(case_analysis),
            "critical_cases_count": len([c for c in case_analysis if c["criticality_score"] > 100]),
            "average_criticality_score": float(np.mean([c["criticality_score"] for c in case_analysis])),
            "severity_distribution": severity_distribution,
            "support_level_distribution": support_level_distribution,
            "frustration_statistics": {
                "high_frustration_cases": claude_statistics["high_frustration"],
                "medium_frustration_cases": claude_statistics["medium_frustration"],
                "low_frustration_cases": claude_statistics["low_frustration"],
                "no_frustration_cases": claude_statistics["no_frustration"],
                "average_frustration_score": claude_statistics["avg_frustration_score"],
            },
            "case_age_statistics": {
                "average_age_days": float(np.mean([c["case_age_days"] for c in case_analysis])),
                "oldest_case_days": int(max([c["case_age_days"] for c in case_analysis])),
                "cases_over_30_days": len([c for c in case_analysis if c["case_age_days"] > 30]),
            },
        }
        
        ai_analysis_summary = {
            "analysis_date": current_date.strftime("%Y-%m-%d"),
            "account_name": customer_name,  # NEW
            "account_health_score": round(account_health_score, 1),  # NEW
            "methodology": {
                "stage_1": "Claude 3.5 Haiku - All cases",
                "stage_2a": f"Claude 3.5 Sonnet Quick Scoring - Top {deepseek_statistics['total_scored']} cases",
                "stage_2b": f"Claude 3.5 Sonnet Detailed Timelines - {deepseek_statistics['total_analyzed']} cases (hybrid: ≥180 OR top 10% outliers)"
            },
            "claude_statistics": claude_statistics,
            "deepseek_statistics": deepseek_statistics,
            "total_analysis_time_seconds": total_time,
            "frustration_distribution": {
                "high": claude_statistics["high_frustration"],
                "medium": claude_statistics["medium_frustration"],
                "low": claude_statistics["low_frustration"],
                "none": claude_statistics["no_frustration"],
            },
            "issue_class_summary": {
                "systemic": systemic_issues,
                "environmental": environmental_issues,
                "component": len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Component']),
                "procedural": len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Procedural']),
            },
            "resolution_outlook_summary": {
                "challenging": challenging_resolutions,
                "manageable": len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Manageable']),
                "straightforward": len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Straightforward']),
            },
        }
        
        top_25_cases_simplified = {
            "analysis_date": current_date.strftime("%Y-%m-%d"),
            "account_name": customer_name,  # NEW
            "account_health_score": round(account_health_score, 1),  # NEW
            "methodology": "Three-Stage Analysis (Claude Haiku + Claude Sonnet Quick Scoring + Claude Sonnet Detailed Timelines)",
            "scoring_explanation": {
                "claude_haiku_frustration": "0-100pts with logarithmic curve - relationship health assessment",
                "issue_class": "5-30pts based on problem type (Systemic=30, Environmental=15, Component=10, Procedural=5)",
                "resolution_outlook": "0-15pts based on complexity (Challenging=15, Manageable=8, Straightforward=0)",
                "other_factors": "Severity, volume, age, support level",
                "claude_sonnet_quick_score": "Pattern analysis on top 25: (Frustration Rate × 100) + (Damage Rate × 50) + Priority Bonus",
                "claude_sonnet_detailed": "Full timeline analysis for cases scoring ≥180"  # UPDATED
            },
            "claude_statistics": claude_statistics,
            "deepseek_statistics": deepseek_statistics,
            "total_analysis_time_seconds": total_time,
            "frustration_distribution": {
                "high": claude_statistics["high_frustration"],
                "medium": claude_statistics["medium_frustration"],
                "low": claude_statistics["low_frustration"],
                "none": claude_statistics["no_frustration"],
            },
            "issue_class_summary": {
                "systemic": systemic_issues,
                "environmental": environmental_issues,
                "component": len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Component']),
                "procedural": len([c for c in case_analysis if c['claude_analysis'].get('issue_class') == 'Procedural']),
            },
            "resolution_outlook_summary": {
                "challenging": challenging_resolutions,
                "manageable": len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Manageable']),
                "straightforward": len([c for c in case_analysis if c['claude_analysis'].get('resolution_outlook') == 'Straightforward']),
            },
        }
        
        top_25_cases_simplified = {
            "analysis_date": current_date.strftime("%Y-%m-%d"),
            "methodology": "Three-Stage Analysis (Claude Haiku + Claude Sonnet Quick Scoring + Claude Sonnet Detailed Timelines)",
            "scoring_explanation": {
                "claude_haiku_frustration": "0-100pts with logarithmic curve - relationship health assessment",
                "issue_class": "5-30pts based on problem type (Systemic=30, Environmental=15, Component=10, Procedural=5)",
                "resolution_outlook": "0-15pts based on complexity (Challenging=15, Manageable=8, Straightforward=0)",
                "other_factors": "Severity, volume, age, support level",
                "claude_sonnet_quick_score": "Pattern analysis on top 25: (Frustration Rate × 100) + (Damage Rate × 50) + Priority Bonus",
                "claude_sonnet_detailed": "Full timeline analysis on top 10 only"
            },
            "top_25_critical_cases": [
                {
                    "rank": i + 1,
                    "case_number": case["case_number"],
                    "customer_name": case["customer_name"],
                    "final_criticality_score": round(case["criticality_score"], 1),
                    "claude_raw_frustration": case['claude_analysis']['frustration_score'],
                    "claude_curved_points": case['score_breakdown']['claude_frustration'],
                    "issue_class": case['claude_analysis'].get('issue_class', 'Unknown'),
                    "issue_class_points": case['score_breakdown']['issue_class'],
                    "resolution_outlook": case['claude_analysis'].get('resolution_outlook', 'Unknown'),
                    "resolution_outlook_points": case['score_breakdown']['resolution_outlook'],
                    "deepseek_quick_score": case['score_breakdown'].get('deepseek_quick_score', 0),
                    "deepseek_frustration_rate_pct": case['score_breakdown'].get('deepseek_frustration_rate', 0),
                    "deepseek_damage_rate_pct": case['score_breakdown'].get('deepseek_damage_rate', 0),
                    "deepseek_priority": case.get('deepseek_quick_scoring', {}).get('priority', 'N/A'),
                    "support_level": case.get('support_level', 'Unknown'),
                    "has_detailed_timeline": case.get('deepseek_analysis') is not None,
                    "timeline_entries_count": len(case.get('deepseek_analysis', {}).get('timeline_entries', [])) if case.get('deepseek_analysis') else 0,
                }
                for i, case in enumerate(top_25_critical)
            ],
        }
        
        client.stream_message("✓ Complete!\n")
        client.stream_message(f"Account: {customer_name}\n")
        client.stream_message(f"Total Cases: {len(case_analysis)}\n")
        client.stream_message(f"Account Health Score: {account_health_score:.1f}/100\n")
        client.stream_message(f"Critical cases (>100): {len([c for c in case_analysis if c['criticality_score'] > 100])}\n")
        client.stream_message(f"Cases with timelines (≥180): {deepseek_statistics['total_analyzed']}\n")
        client.stream_message(f"Systemic issues: {systemic_issues}\n")
        client.stream_message(f"Challenging resolutions: {challenging_resolutions}\n\n")
        
        return AgentResponse(
            analysis_complete=f"Account health assessment complete for {customer_name}: {len(case_analysis)} cases analyzed. Health Score: {account_health_score:.1f}/100. High frustration: {claude_statistics['high_frustration']}, Systemic issues: {systemic_issues}, Challenging resolutions: {challenging_resolutions}. Timeline analysis: {deepseek_statistics['total_analyzed']} cases (hybrid selection: ≥180 OR top 10% outliers).",
            critical_cases_json=Blob(
                json.dumps(critical_cases_analysis, indent=2).encode("utf-8"),
                "application/json",
                filename="critical_cases_hybrid.json",
            ),
            summary_stats_json=Blob(
                json.dumps(summary_statistics, indent=2).encode("utf-8"),
                "application/json",
                filename="summary_statistics_hybrid.json",
            ),
            frustration_analysis_json=Blob(
                json.dumps(ai_analysis_summary, indent=2).encode("utf-8"),
                "application/json",
                filename="ai_analysis_summary.json",
            ),
            top_25_cases_json=Blob(
                json.dumps(top_25_cases_simplified, indent=2).encode("utf-8"),
                "application/json",
                filename="top_25_critical_cases_hybrid.json",
            ),
            frustration_chart=Blob(
                charts['frustration_distribution'],
                "image/png",
                filename="frustration_distribution_hybrid.png",
            ),
            issue_categories_chart=Blob(
                charts['issue_categories'],
                "image/png",
                filename="issue_categories_distribution.png",
            ),
            score_breakdown_chart=Blob(
                charts['score_breakdown'],
                "image/png",
                filename="score_breakdown_hybrid.png",
            ),
            severity_distribution_chart=Blob(
                charts['severity_distribution'],
                "image/png",
                filename="severity_distribution.png",
            ),
            support_level_distribution_chart=Blob(
                charts['support_level_distribution'],
                "image/png",
                filename="support_level_distribution.png",
            ),
            top_25_critical_chart=Blob(
                charts['top_25_critical'],
                "image/png",
                filename="top_25_critical_hybrid.png",
            ),
            frustration_trend_chart=Blob(
                charts.get('frustration_trend', b''),
                "image/png",
                filename="frustration_trend_over_time.png",
            ),
            case_volume_trend_chart=Blob(
                charts.get('case_volume_trend', b''),
                "image/png",
                filename="case_volume_trend_over_time.png",
            ),
            comprehensive_report=Blob(
                pdf_bytes,
                "application/pdf",
                filename="customer_sentiment_hybrid_report.pdf",
            ),
        )
        
    except Exception as e:
        client.stream_message(f"\n✗ Error: {str(e)}\n")
        import traceback
        client.stream_message(traceback.format_exc())
        
        return AgentResponse(
            analysis_complete=f"Analysis failed: {str(e)}",
            critical_cases_json=None,
            summary_stats_json=None,
            frustration_analysis_json=None,
            top_25_cases_json=None,
            frustration_chart=None,
            issue_categories_chart=None,
            score_breakdown_chart=None,
            severity_distribution_chart=None,
            support_level_distribution_chart=None,
            top_25_critical_chart=None,
            comprehensive_report=None,
        )
